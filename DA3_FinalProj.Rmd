---
title:  |
  | \vspace{5cm} Stat 8330 Final Project Report
author: "Katie Price, Emily Scully, Ben Graves, Ellen Fitzsimmons, and Mira Isnainy"
date: \today
output: pdf_document
indent: true
header-includes:
  - \usepackage{pdfpages}
  - \usepackage{indentfirst}
  - \usepackage{float}
---

\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\proglang=\textsf
\let\code=\texttt

\newpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("ncdf4")
#install.packages("raster")
#install.packages("rgdal")
#install.packages("ggplot2")
#install.packages("ncdf4.helpers")
#install.packages("PCICt")
library(ncdf4) # package for netcdf manipulation
library(raster) # package for raster manipulation
library(rgdal) # package for geospatial analysis
library(ggplot2) # package for plotting
library(ncdf4.helpers)
library(PCICt)
library(lubridate)
library(dplyr)
library(maps)
library(fields)
library(tidyr)
library(glmnet)
library(randomForest)
library(BART)
library(xtable)
library(pls)
library(gam) ## GAMs
options(xtable.comment = FALSE)


library(knitr)
library(ggpubr)
library(tidyverse)
library(tibble)
library(tfdatasets)
library(keras)
library(reshape2)

library(kernlab)
library(NMF)
library(lle)
library(dimRed)
library(loe)
library(RSpectra)

load("precip_region_cont_df_wide.RData")

```

```{r data, echo=FALSE}
## import data
# To load the RData
load("data_in_dfs.RData")

```

# Introduction
Meteorologists study long and hard to predict weather patterns, day in and day out. Anticipating precipitation in North America is not only a convenience for the general public, but a necessity for farmers and agriculturalists. The importance of clean water is a secret to no one. Clean water is not only a key factor in the survival of nearly all living species, but also a key resource for power and production in the modern world. From food production to electricity, water can be used in many different forms for many important aspects of the human experience. Predicting precipitation and subsequently predicting to some extent the frequency and volume of available freshwater is a valuable and essential analysis. This study looks to examine the relationship between sea surface temperatures (henceforth SST) patterns in the tropical Pacific Ocean and precipitation over North America. It was our desire to be able to predict precipitation levels based on SST, both contemporaneously and on a future-time basis. We used several different methods to predict the precipitation outcomes as accurately as possible, including: **insert all methods here**. 

# Exploratory Data Analysis

The data available includes  monthly SST anomalies on a grid for the period January 1948 through February 2018. These "anomalies" consist of differences of the temperature from a long-term average. Additionally, the data includes precipitation data for a majority of the North American continent. Examples of each data source are plotted below for May 2017:

```{r, echo = FALSE, out.width="60%", fig.align='center'}

state_map = map_data("state")
bmap = map_data("state")


# Fancy plots
precipPlot = ggplot() +
  coord_fixed(ratio = 1) +
  geom_raster(data = Pdat_df,
              aes(x = long, y = lat, fill = May2017),
              alpha = 1) +
  geom_polygon(
    data = bmap,
    aes(x = long, y = lat, group = group),
    inherit.aes = F,
    colour = 'black',
    fill = NA,
    lwd = 0.5
  ) +
  scale_fill_gradientn(
    na.value = "white",
    # Limits need to be updated accordingly
    limits = c(min(Pdat_df$May2017) - 0.5, 
               max(Pdat_df$May2017) + 0.5),
    colours = c("blue", "green", "orange", "yellow")
  ) +
  theme_classic() + 
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank()
  ) + 
  labs(fill = "Precipication \nUnits", 
       title = "May 2017 Observed Precipitation")


SSTPlot = ggplot() +
  coord_fixed(ratio = 1) +
  geom_raster(data = SST_df,
              aes(x = long, y = lat, fill = May2017),
              alpha = 1) +
  scale_fill_gradientn(
    na.value = "white",
    # Limits need to be updated accordingly
    limits = c(min(Pdat_df$May2017) - 0.5, 
               max(Pdat_df$May2017) + 0.5),
    colours = c("blue", "green", "orange", "yellow")
  ) +
  theme_classic() + 
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank()
  ) + 
  labs(fill = "Temperature \nUnits", 
       title = "May 2017 Observed SST")


gridExtra::grid.arrange(precipPlot, SSTPlot)

```

It should be noted that for the purpose of this research inherent time and spatial relationships were ignored, so data from each month and year was assumed to be independent of the next.

# Dimension Reduction and Clustering

In order to decrease dimensionality of the data set, we conduct a series of dimension reduction techniques and then use these results to create location based clusters. For dimension reduction of both the SST and precipitation data, we consider principal components analysis using the singular value decomposition (SVD), kernel PCA (KPCA), local linear embedding (LLE), and Laplacian Eigenmaps (LE). The number of principal components for the two PCAs are selected by assessing cumulative variance accounted for, comparing to randomized data, dimensional minimization, and impact on clustering clarity. Other factors such as kernel smoothing method for the KPCA and number of neighbors for LLE are also optimized. 

Determination of the number of regional k-means clusters for each of the four dimensional reduction methods above is done with k-means clustering and aided with the \pkg{NbClust} package. This package uses 30 different indices to suggest an appropriate number of clusters and varies all combinations of number of clusters, distance measures, and clustering methods. The final number of clusters is decided using a 'majority rule' approach and looking at cluster clarity and complexity. Basically, the number of clusters with the most indices in agreement were considered and then of those the one with the most visually clear clusters when plotted were chosen. 

Results from each of the dimensional reduction methods and clustering are presented below for the SST and precipitation data sets. 

## SST Results

```{r SSTSVDsetup, include=FALSE, cache=TRUE}
SST_df <- na.omit(SST_df)
SST_df <- apply(SST_df, 2, as.numeric)
LongLat <- SST_df[, 1:2]
MonthlySST <- SST_df[ , -c(1,2)]

## For ggplots
ggform <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
              panel.background = element_blank(), axis.line = element_line(colour = "black"))

## Start with SVD to get an idea of Prin Comp needed
SSTSVD <- svd(MonthlySST)
SSTsingvals <- matrix(NA, nrow = 842, ncol = 2)
SSTsingvals[, 1] <- 1:842
SSTsingvals[, 2] <- SSTSVD$d
SVDvaracc <- cumsum(SSTSVD$d[1:33]^2)/sum(SSTSVD$d^2)*100


## Set up random data
set.seed(938837)
rand <- matrix(NA, 2512, 842)
for (i in 1:842) {
    rand[ , i] <- MonthlySST[sample(nrow(MonthlySST)), i]
}

randdecomp <- svd(rand)
SSTsingvals <- data.frame(SSTsingvals, as.numeric(randdecomp$d))
colnames(SSTsingvals) <- c('Component', 'Real Data Trace', 'Random Data Trace')

## PC DF
SSTUD <- SSTSVD$u %*% diag(SSTSVD$d)
SSTUD <- data.frame(LongLat, SSTUD[ , -c(34:842)])
colnames(SSTUD) <- c('Longitude', 'Latitude', paste0('PC', 1:33))

## Cluster
set.seed(662321)
SVDclustres <- kmeans(SSTUD[, -c(1, 2)], 2, nstart = 20, iter.max = 15)
SVDclustout <- data.frame(LongLat, Cluster = factor(SVDclustres$cluster))
colnames(SVDclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

```{r loadothers, include=FALSE}
if (file.exists('SSTdimreduce.RData')){
    load('SSTdimreduce.RData')
} else {
    kpca2 <- kpca(MonthlySST, kernel = "vanilladot", kpar = list())
    kpcarand <- kpca(rand, kernel = "vanilladot", kpar = list())
    lleres <- lle(MonthlySST, m = 2, k = 16)
    LE <- embed(MonthlySST, .method = 'LaplacianEigenmaps')
    save(kpca2, kpcarand, lleres, LE, file = 'SSTdimreduce.RData')
}
```

First, the SST data set is converted into principal components (PCs) using SVD. The number of important components is assessed by looking at the number of components with singular values above those that are generated from a randomized version of the data set. Singular values for each component for the true and random data set can be seen in Figure \ref{fig:eigenfig}. This suggests using 33 PCs which accounts for a total variance of `r round(SVDvaracc[33], 2)`%with the first component accounting for `r round(SVDvaracc[1], 2)`%. Figure \ref{fig:SSTPC1s} plots the weights associated with the first PC on the latitude and longitude values. 

Of the indices for determining the number of clusters from the suggested 33 PCs, eight suggest two clusters, seven suggest three clusters, and seven suggest 20 clusters. Inspection of the clusters when plotted with latitude and longitude show a close similarity between having 2 and 3 clusters. The two cluster solution can be seen in Figure \ref{fig:SSTClusters} The addition of another cluster just adds another area around the central cluster in the two cluster solution. Using 20 clusters seems a little excessive in this case and lead to some very small areas that belonged to one cluster but were not near the main cluster. One point of note is that this clustering closely resembles the distribution of the first PC.

```{r SSTKPCA, include = FALSE}
#kpca2 <- kpca(MonthlySST, kernel = "vanilladot", kpar = list()) 
SSTkpcavar <- cumsum(kpca2@eig[1:32]^2)/sum(kpca2@eig^2)*100
#kpcarand <- kpca(rand, kernel = "vanilladot", kpar = list())
kpcaeig <- data.frame(Component = 1:162, Real = kpca2@eig, Random = kpcarand@eig[1:162])
colnames(kpcaeig) <- c('Component', 'Real Data Trace', 'Random Data Trace')
kpcaeig<- melt(kpcaeig, id.vars = 'Component', 
                    variable.name = 'Data', value.name = 'EigenVal')


KPCApcs <- kpca2@pcv[ , 1:6]
KPCApcs <- data.frame(LongLat, KPCApcs)
colnames(KPCApcs) <- c('Longitude', 'Latitude', paste0('PC', 1:6))

set.seed(662321)
## Suggests 5
KPCAclustres <- kmeans(KPCApcs[, -c(1,2)], 5, nstart = 20, iter.max = 15) 
KPCAclustout <- data.frame(LongLat, Cluster = factor(KPCAclustres$cluster))
colnames(KPCAclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

A similar procedure was followed for the KPCA except multiple kernels were also assessed. Of these, the linear kernel function seems to perform the most efficiently. It takes fewer PCs and less tuning to account for a similar amount of variance as the other kernels do. Similarly, when comparing to random data, 32 components are suggested this time (Figure \ref{fig:eigenfig}). However, if we use the SVD as a base line, it is possible that only 6 components are needed because they account for `r round(SSTkpcavar[6], 2)`% of the variance. Figure \ref{fig:SSTPC1s} presents a plot of the first PC for the KPCA. 

Using these six components for clustering, seven indices suggest five as the best number of clusters, four suggest two clusters, and 3 suggest 6 or 20 clusters. So, five clusters seems to have the most support. The cluster structure here seems to break up the northern and eastern most sections of the pacific more with a bit of an alternating pattern, leaving a single cluster for the southwest area. The clustering can be seen in Figure \ref{fig:SSTClusters}.

```{r eigenfigs, echo=FALSE, results='asis', fig.cap = '\\label{fig:eigenfig} Plot of singular values and Eigen values for real and randomized SST data set for PCA and KPCA respectively. The PCs of the start performing worse than random after component 33 for the PCA and 32 for KPCA.', fig.align = 'center', out.width='75%', fig.pos="H"}
## Suggests 33 PCs (97% Var)
SSTsingvals <- melt(SSTsingvals, id.vars = 'Component', 
                    variable.name = 'Data', value.name = 'SingVal')

singvalplot <- ggplot(SSTsingvals, aes(x = Component, y = SingVal, color = Data)) + 
               geom_line() + ggform + xlab('Principal Component') + ylab('Singular Value') + labs(fill = 'Data Set') + 
               theme(legend.position = c(.8, .8)) + ggtitle('PCA Singular Values')
    

    
## If legend needs resizing
# theme(legend.position = c(.8, .8), legend.key.size = unit(1, 'cm'),
#                                          legend.key.height = unit(1, 'cm'), 
#                                          legend.key.width = unit(1, 'cm'), 
#                                          legend.title = element_text(size=14), 
#                                          legend.text = element_text(size=12)) 

## Suggests 33 PCs (97% Var)
eigenplot <- ggplot(kpcaeig, aes(x = Component, y = EigenVal, color = Data)) + 
             geom_line() + ggform + xlab('Principal Component') + ylab('Eigen Value') + labs(fill = 'Data Set') + 
             theme(legend.position = c(.8, .8)) + ggtitle('KPCA Eigen Values')

ggarrange(singvalplot, eigenplot, ncol = 1, nrow = 2)
```

```{r SSTPC1, echo=FALSE, results='asis', fig.cap = '\\label{fig:SSTPC1s} Plot of PC1 from PCA and KPCA for SST data.', fig.align = 'center', out.width='65%', fig.pos="H"}
## Plot PC 1
SSTSVDPC1 <- ggplot(SSTUD, aes(x = Longitude, y = Latitude, fill = PC1)) + 
             coord_fixed(ratio = 1) + geom_raster(alpha = 1) + ggform + 
             scale_fill_gradientn(na.value = "white", colours = c("yellow", "orange", "green", "blue")) +
             ggtitle('PCA PC1')

## Plot PC 1
SSTkpcaPC1 <- ggplot(KPCApcs, aes(x = Longitude, y = Latitude, fill = PC1)) + 
              coord_fixed(ratio = 1) + geom_raster(alpha = 1) + ggform + 
              scale_fill_gradientn(na.value = "white", colours = c("yellow", "orange", "green", "blue")) +
              ggtitle('KPCA PC1')

ggarrange(SSTSVDPC1, SSTkpcaPC1, ncol = 1, nrow = 2)
```

```{r SSTLLE, include=FALSE}
# lleres <- lle(MonthlySST, m = 2, k = 16)
# LE <- embed(MonthlySST, .method = 'LaplacianEigenmaps')

Ymat <- data.frame(lleres$Y)
LEout <- data.frame(Obs = 1:2512, LE@data@data)

set.seed(662321)
LLEclustres <- kmeans(Ymat, 10, nstart = 20, iter.max = 15)
LLEclustout <- data.frame(LongLat, Cluster = factor(LLEclustres$cluster))
colnames(LLEclustout) <- c('Longitude', 'Latitude', 'Cluster')

set.seed(662321)
LEclustres <- kmeans(LEout[, -1], 3, nstart = 20, iter.max = 15) ## suggests 2 or 3 clusters
LEclustout <- factor(LEclustres$cluster)
LEclustout <- data.frame(LongLat, Cluster = factor(LEclustres$cluster))
colnames(LEclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

Dimensional reduction was also attempted using LLE and Laplacian Eigenmaps. For LLE, the optimum number of neighbors found for two intrinsic dimensions is 16. The proposed number of clusters by the indices is 10. This was suggest by six indices, followed closely by three clusters with five indices and two clusters with four indices. This is by far the most chaotic clustering seen in Figure \ref{fig:SSTClusters}. Many of the clusters are surround by other clusters.

The Laplacian Eigenmap also reduced dimensionality down to two. Clustering indices suggest that the optimal number of clusters is three with six indices. Two and four clusters could also be considered with 4 indices proposing each of those. Using three clusters, the Pacific is split into a single cluster to the west and two clusters in the east being split almost at the equator (Figure \ref{fig:SSTClusters}). 

```{r clusterplot1, echo = FALSE, results='asis', fig.cap = '\\label{fig:SSTClusters} Best clustering solutions for each of the dimension reduction methods for SST data.', fig.align = 'center', fig.height=9, fig.pos="H"}
## SVD plot
SSTSVDClust <- ggplot(SVDclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
               coord_fixed(ratio = 1) + geom_raster(alpha = 1) + ggform + ggtitle('Clustering from PCA') +
                theme(legend.key.size = unit(.5, 'cm'),
                     legend.key.height = unit(.5, 'cm'),
                     legend.key.width = unit(.5, 'cm'),
                     legend.title = element_text(size=10),
                     legend.text = element_text(size=8)) 

## KPCA plot
SSTKPCAClust <- ggplot(KPCAclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) +
                coord_fixed(ratio = 1) + geom_raster(alpha = 1) + ggform + ggtitle('Clustering from KPCA') +
                theme(legend.key.size = unit(.5, 'cm'),
                     legend.key.height = unit(.5, 'cm'),
                     legend.key.width = unit(.5, 'cm'),
                     legend.title = element_text(size=10),
                     legend.text = element_text(size=8)) 

## LLE plot
SSTLLEClust <- ggplot(LLEclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
               coord_fixed(ratio = 1) + geom_raster(alpha = 1) + ggform + ggtitle('Clustering from LLE') +
               theme(legend.key.size = unit(.5, 'cm'),
                     legend.key.height = unit(.5, 'cm'),
                     legend.key.width = unit(.5, 'cm'),
                     legend.title = element_text(size=10),
                     legend.text = element_text(size=8)) + guides(fill=guide_legend(ncol=2))

## LE plot
SSTLEClust <- ggplot(LEclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
              coord_fixed(ratio = 1) + geom_raster(alpha = 1) + ggform + ggtitle('Clustering from LE') +
              theme(legend.key.size = unit(.5, 'cm'),
                     legend.key.height = unit(.5, 'cm'),
                     legend.key.width = unit(.5, 'cm'),
                     legend.title = element_text(size=10),
                     legend.text = element_text(size=8)) 

ggarrange(SSTSVDClust, SSTKPCAClust, SSTLLEClust, SSTLEClust, ncol = 1, nrow = 4)
```

```{r clustbypc, echo = FALSE, results='asis', fig.cap = '\\label{fig:SSTClustbypc} Location clusters plotted by the first two PCs or manifolds of each dimension reduction techinque.', fig.align = 'center', fig.pos="H"}
SSTSVDPCplot <- ggplot(data = data.frame(SSTUD, SVDclustout), aes(x = PC1, y = PC2, color = Cluster)) + 
                geom_point(size = .3) + ggform + xlab('Principal Component 1') + 
                ylab('Principal Component 2') + ggtitle('PCA Clusters') +
                theme(legend.key.size = unit(.25, 'cm'),
                     legend.key.height = unit(.25, 'cm'),
                     legend.key.width = unit(.25, 'cm'),
                     legend.title = element_text(size=8),
                     legend.text = element_text(size=6)) 

SSTKPCAPCplot <- ggplot(data = data.frame(KPCApcs, KPCAclustout), 
                        aes(x = PC1, y = PC2, color = Cluster)) + 
                 geom_point(size = .3) + ggform + xlab('Principal Component 1') + 
                 ylab('Principal Component 2') + ggtitle('KPCA Clusters') +
                 theme(legend.key.size = unit(.25, 'cm'),
                     legend.key.height = unit(.25, 'cm'),
                     legend.key.width = unit(.25, 'cm'),
                     legend.title = element_text(size=8),
                     legend.text = element_text(size=6)) 

SSTLLEdimplot <- ggplot(data = data.frame(Ymat, LLEclustout), aes(x = Ymat[, 1], y = Ymat[, 2], color = Cluster)) +
                 geom_point(size = .3) + ggform + xlab('Dimension 1') + 
                 ylab('Dimension 2') + ggtitle('LLE Clusters') +
                 theme(legend.key.size = unit(.25, 'cm'),
                     legend.key.height = unit(.25, 'cm'),
                     legend.key.width = unit(.25, 'cm'),
                     legend.title = element_text(size=8),
                     legend.text = element_text(size=6)) + guides(fill=guide_legend(ncol=2))

SSTLEmanifolds <- ggplot(data = data.frame(LEout, LEclustout), aes(x = LEout[, 1], y = LEout[, 2], color = Cluster)) +
                  geom_point(size = .3) + ggform + xlab('Manifold 1') + 
                  ylab('Manifold 2') + ggtitle('LE Clusters') +
                  theme(legend.key.size = unit(.25, 'cm'),
                     legend.key.height = unit(.25, 'cm'),
                     legend.key.width = unit(.25, 'cm'),
                     legend.title = element_text(size=8),
                     legend.text = element_text(size=6)) 

ggarrange(SSTSVDPCplot, SSTKPCAPCplot, SSTLLEdimplot, SSTLEmanifolds, ncol = 2, nrow = 2)
```

```{r cleanup, include=FALSE}
remove(kpca2, kpcarand, lleres, LE, SSTSVD, randdecomp, MonthlySST, rand)
```

## Precipitation Results

```{r precipsetup, include=FALSE}
Pdat_df <- na.omit(Pdat_df)
Pdat_df <- apply(Pdat_df, 2, as.numeric)
LongLat2 <- Pdat_df[, 1:2]
MonthlyPrecip <- Pdat_df[ , -c(1,2)]

## Start with SVD to get an idea of Prin Comp needed
PrecipSVD <- svd(MonthlyPrecip)
Precipsingvals <- matrix(NA, nrow = 842, ncol = 2)
Precipsingvals[, 1] <- 1:842
Precipsingvals[, 2] <- PrecipSVD$d
PrecipSVDvaracc <- cumsum(PrecipSVD$d[1:26]^2)/sum(PrecipSVD$d^2)*100

## Set up random data
set.seed(374378)
rand2 <- matrix(NA, 5390, 842)
for (i in 1:842) {
    rand2[ , i] <- MonthlyPrecip[sample(nrow(MonthlyPrecip)), i]
}

rand2decomp <- svd(rand2)
Precipsingvals <- data.frame(Precipsingvals, as.numeric(rand2decomp$d))
colnames(Precipsingvals) <- c('Component', 'Real Data Trace', 'Random Data Trace')


## set up PCs
PrecipUD <- PrecipSVD$u %*% diag(PrecipSVD$d)
PrecipUD <- data.frame(LongLat2, PrecipUD[ , -c(27:842)])
colnames(PrecipUD) <- c('Longitude', 'Latitude', paste0('PC', 1:26))

## Cluster
set.seed(629787)
PSVDclustres <- kmeans(PrecipUD[, -c(1, 2)], 4, nstart = 20, iter.max = 20) ## 2 or 4 suggested as best
PSVDclustout <- data.frame(LongLat2, Cluster = factor(PSVDclustres$cluster))
colnames(PSVDclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

```{r loadothers2, include=FALSE}
if (file.exists('Precipdimreduce.RData')){
    load('Precipdimreduce.RData')
} else {
    Pkpca2 <- kpca(MonthlySST, kernel = "vanilladot", kpar = list())
    Pkpcarand <- kpca(rand, kernel = "vanilladot", kpar = list())
    Plleres <- lle(MonthlySST, m = 2, k = 16)
    PLE <- embed(MonthlySST, .method = 'LaplacianEigenmaps')
    save(Pkpca2, Pkpcarand2, Plleres, PLE, file = 'Precipdimreduce.RData')
}
```

The procedures for dimensional reduction for the precipitation data are similar to those above. Likewise, the number of important components is assessed by comparing the singular values of each PC from the real data to those that come from a randomized version of the data set. This suggests that 26 PCs should be considered. The PCs account for a total of `r round(PrecipSVDvaracc[26], 2)`% of the variance. The first PC accounts for `r round(PrecipSVDvaracc[1], 2)`%. Figure \ref{fig:eigenfig2} shows the comparison of the singular values and Figure \ref{fig:PrecipPC1s} shows a plot of the first PC. 

The important PCs were once again used to create regional clusters for the precipitation data. Eight clustering indices suggest two as the best number of clusters, seven propose 4 clusters, and five propose 15. When using two clusters, the United States is split into a group containing the Pacific north west and states near the Mississippi river and eastward and a group containing the rest of the west. A four cluster solution more closely resembles the first principal component. Similar to the SST data set. This is seen in Figure \ref{fig:PrecipClusters}. 

```{r precipkpca, include=FALSE}
#Pkpca2 <- kpca(MonthlyPrecip, kernel = "vanilladot", kpar = list()) ##maybe 2-4 PCs? Better than Random suggests 25.
Pkpcavaracc <- cumsum(Pkpca2@eig[1:26]^2)/sum(Pkpca2@eig^2)*100
#Pkpcarand2 <- kpca(rand2, kernel = "vanilladot", kpar = list())
Pkpcaeig <- data.frame(Component = 1:842, Real = Pkpca2@eig, Random = Pkpcarand2@eig)
colnames(Pkpcaeig) <- c('Component', 'Real Data Trace', 'Random Data Trace')
Pkpcaeig<- melt(Pkpcaeig, id.vars = 'Component', 
                    variable.name = 'Data', value.name = 'EigenVal')

PKPCApcs <- Pkpca2@pcv[ , 1:4]

## Cluster
## 12 Suggested
set.seed(662321)
PKPCAclustres <- kmeans(PKPCApcs, 12, nstart = 20, iter.max = 15)
PKPCAclustout <- data.frame(LongLat2, Cluster = factor(PKPCAclustres$cluster))
colnames(PKPCAclustout) <- c('Longitude', 'Latitude', 'Cluster')

PKPCApcs <- data.frame(LongLat2, PKPCApcs)
colnames(PKPCApcs) <- c('Longitude', 'Latitude', paste0('PC', 1:4))
```

The KPCA model that provided the largest dimensional reduction without having to tune applies a linear kernel. When compared to the randomized data set, 25 PCs are suggested to be optimal. This accounts for `r round(Pkpcavaracc[25], 2)`% of the total variance. However, the first two components account for `r round(Pkpcavaracc[2], 2)`% and first four have `r round(Pkpcavaracc[4], 2)`%. So, if it isn't important to account for almost all the variance, the dimension could be reduced much further. Once again, the plot of the Eigen values can be seen in Figure \ref{fig:eigenfig2} and a plot of the first PC in Figure \ref{fig:PrecipPC1s}. 

Using the first four PCs from the KPCA the optimal number of clusters was assessed again. Seven indices suggest 12 clusters are appropriate and five indices suggest two or three clusters are appropriate. For the most part, the twelve clusters do break up the U.S. into specific regions. However, there are a few smaller areas that are split apart from the main cluster. For example, a portion of the Carolinas, Virginia, and Maryland area are clustered with the west and southwest region (Figure \ref{fig:PrecipClusters}).

```{r eigenfigs2, echo=FALSE, results='asis', fig.cap = '\\label{fig:eigenfig2} Plot of singular values and Eigen values for real and randomized precipitation data set for PCA and KPCA respectively. The PCs of the start performing worse than random after component 33 for the PCA and 32 for KPCA.', fig.align = 'center', out.width='75%', fig.pos="H"}
## Suggests 26 PCs (97% Var)
Precipsingvals <- melt(Precipsingvals, id.vars = 'Component', 
                    variable.name = 'Data', value.name = 'SingVal')

singvalplot <- ggplot(Precipsingvals[c(1:400, 843:1243),], aes(x = Component, y = SingVal, color = Data)) + 
               geom_line() + ggform + xlab('Principal Component') + ylab('Singular Value') + labs(fill = 'Data Set') + 
               theme(legend.position = c(.8, .8)) + ggtitle('PCA Singular Values')
    


## Suggests 25 PCs (97% Var)
eigenplot <- ggplot(Pkpcaeig[c(1:400, 843:1243),], aes(x = Component, y = EigenVal, color = Data)) + 
             geom_line() + ggform + xlab('Principal Component') + ylab('Eigen Value') + labs(fill = 'Data Set') + 
             theme(legend.position = c(.8, .8)) + ggtitle('KPCA Eigen Values')

ggarrange(singvalplot, eigenplot, ncol = 1, nrow = 2)
```

```{r PrecipPC1, echo=FALSE, results='asis', fig.cap = '\\label{fig:PrecipPC1s} Plot of PC1 from PCA and KPCA for precipitation data.', fig.align = 'center', out.width='65%', fig.pos="H"}
bmap = map_data("state")
colnames(bmap)[1:2] <- c('Longitude', 'Latitude')

## Plot PC 1
PrecipSVDPC1 <- ggplot(PrecipUD, aes(x = Longitude, y = Latitude, fill = PC1)) + 
                coord_fixed(ratio = 1) + 
                geom_raster(alpha = 1) + 
                geom_polygon(data = bmap,
                             aes(x = Longitude, y = Latitude, group = group),
                             inherit.aes = F, colour = 'black', 
                             fill = NA, lwd = 0.5) + 
                ggform + scale_fill_gradientn(na.value = "white", 
                                            colours = c("yellow", "orange", "green", "blue")) +
                ggtitle('PCA PC1')

## Plot PC 1
PrecipkpcaPC1 <- ggplot(PKPCApcs, aes(x = Longitude, y = Latitude, fill = PC1)) + 
                 coord_fixed(ratio = 1) + 
                 geom_raster(alpha = 1) +
                 geom_polygon(data = bmap,
                              aes(x = Longitude, y = Latitude, group = group),
                              inherit.aes = F, colour = 'black', 
                              fill = NA, lwd = 0.5) +
                 ggform + 
                 scale_fill_gradientn(na.value = "white", 
                                      colours = c("yellow", "orange", "green", "blue")) +
                 ggtitle('KPCA PC1')

ggarrange(PrecipSVDPC1, PrecipkpcaPC1, ncol = 1, nrow = 2)
```

```{r precipLLE, include=FALSE}
## Opt K = 25 for 2
#Plleres <- lle(MonthlyPrecip, m = 2, k = 25)
PYmat <- data.frame(Plleres$Y)

## Cluster
set.seed(662321)
PLLEclustres <- kmeans(PYmat, 3, nstart = 20, iter.max = 25) #3 suggested, 2 next
PLLEclustout <- data.frame(LongLat2, Cluster = factor(PLLEclustres$cluster))
colnames(PLLEclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

```{r precipLE, include=FALSE}
#PLE <- embed(MonthlyPrecip, .method = 'LaplacianEigenmaps')
PLEout <- data.frame(Obs = 1:5390, PLE@data@data)


## Cluster
set.seed(662321)
PLEclustres <- kmeans(PLEout[, -1], 3, nstart = 20, iter.max = 20) ## suggests 2 or 3 clusters
PLEclustout <- data.frame(LongLat2, Cluster = factor(PLEclustres$cluster))
colnames(PLEclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

Finally, LLE and LE is also used for the dimension reduction of the precipitation data. The LLE has two intrinsic dimensions and uses 25 nearest neighbor observations. Clustering indices suggest that three is the optimum number of clusters with two clusters being next best. These have seven and six indices suggesting them respectively. Looking at Figure \ref{fig:PrecipClusters}, the clusters are scattered all over the place. This could make explanation difficult for future models.

Clustering indices for the LE suggest that the optimal number of clusters here is also three with an overwhelming majority. The clusters here are much more obvious though and somewhat similar to those of the PCA. It divides the United States into three regions with one being the west coast, one the central US, and the final one being states east of the Mississippi river (Figure \ref{fig:PrecipClusters}). Figure \ref{fig:PrecipClustbypc} shows all of the clusters plotted with the first two dimensions of each of the reduction methods.

For the following analyses, we use the KPCA results for each of the data sets. The SST data makes use of the five cluster solution and the precipitation uses the 12 cluster solution. 

```{r clusterplot2, echo = FALSE, results='asis', fig.cap = '\\label{fig:PrecipClusters} Best clustering solutions for each of the dimension reduction methods for Precipitation data.', fig.align = 'center', fig.height=9, fig.pos="H"}
## SVD plot
PrecipSVDClust <- ggplot(PSVDclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
                  coord_fixed(ratio = 1) + geom_raster(alpha = 1) +
                  geom_polygon(data = bmap,
                             aes(x = Longitude, y = Latitude, group = group),
                             inherit.aes = F, colour = 'black', 
                             fill = NA, lwd = 0.5) +
                  ggform + ggtitle('Clustering from PCA') +
                  theme(legend.key.size = unit(.5, 'cm'),
                        legend.key.height = unit(.5, 'cm'),
                        legend.key.width = unit(.5, 'cm'),
                        legend.title = element_text(size=10),
                        legend.text = element_text(size=8)) 

## KPCA plot
PrecipKPCAClust <- ggplot(PKPCAclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) +
                   coord_fixed(ratio = 1) + geom_raster(alpha = 1) + 
                   geom_polygon(data = bmap,
                             aes(x = Longitude, y = Latitude, group = group),
                             inherit.aes = F, colour = 'black', 
                             fill = NA, lwd = 0.5) +
                   ggform + ggtitle('Clustering from KPCA') +
                   theme(legend.key.size = unit(.5, 'cm'),
                         legend.key.height = unit(.5, 'cm'),
                         legend.key.width = unit(.5, 'cm'),
                         legend.title = element_text(size=10),
                         legend.text = element_text(size=8)) + guides(fill=guide_legend(ncol=2))

## LLE plot
PrecipLLEClust <- ggplot(PLLEclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
                  coord_fixed(ratio = 1) + geom_raster(alpha = 1) + 
                  geom_polygon(data = bmap,
                             aes(x = Longitude, y = Latitude, group = group),
                             inherit.aes = F, colour = 'black', 
                             fill = NA, lwd = 0.5) +
                  ggform + ggtitle('Clustering from LLE') +
                  theme(legend.key.size = unit(.5, 'cm'),
                        legend.key.height = unit(.5, 'cm'),
                        legend.key.width = unit(.5, 'cm'),
                        legend.title = element_text(size=10),
                        legend.text = element_text(size=8))

## LE plot
PrecipLEClust <- ggplot(PLEclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
                 coord_fixed(ratio = 1) + geom_raster(alpha = 1) + 
                 geom_polygon(data = bmap,
                             aes(x = Longitude, y = Latitude, group = group),
                             inherit.aes = F, colour = 'black', 
                             fill = NA, lwd = 0.5) +
                 ggform + ggtitle('Clustering from LE') +
                 theme(legend.key.size = unit(.5, 'cm'),
                       legend.key.height = unit(.5, 'cm'),
                       legend.key.width = unit(.5, 'cm'),
                       legend.title = element_text(size=10),
                       legend.text = element_text(size=8)) 

ggarrange(PrecipSVDClust, PrecipKPCAClust, PrecipLLEClust, PrecipLEClust, ncol = 1, nrow = 4)
```

```{r clustbypc2, echo = FALSE, results='asis', fig.cap = '\\label{fig:PrecipClustbypc} Location clusters plotted by the first two PCs or manifolds of each dimension reduction techinque.', fig.align = 'center', fig.pos="H"}
PrecipSVDPCplot <- ggplot(data = data.frame(PrecipUD, PSVDclustout), aes(x = PC1, y = PC2, color = Cluster)) + 
                   geom_point(size = .3) + ggform + xlab('Principal Component 1') + 
                   ylab('Principal Component 2') + ggtitle('PCA Clusters') +
                   theme(legend.key.size = unit(.25, 'cm'),
                         legend.key.height = unit(.25, 'cm'),
                         legend.key.width = unit(.25, 'cm'),
                         legend.title = element_text(size=8),
                         legend.text = element_text(size=6)) 

PrecipKPCAPCplot <- ggplot(data = data.frame(PKPCApcs, PKPCAclustout), 
                        aes(x = PC1, y = PC2, color = Cluster)) + 
                    geom_point(size = .3) + ggform + xlab('Principal Component 1') + 
                    ylab('Principal Component 2') + ggtitle('KPCA Clusters') +
                    theme(legend.key.size = unit(.25, 'cm'),
                          legend.key.height = unit(.25, 'cm'),
                          legend.key.width = unit(.25, 'cm'),
                          legend.title = element_text(size=8),
                          legend.text = element_text(size=6)) 

PrecipLLEdimplot <- ggplot(data = data.frame(PYmat, PLLEclustout), aes(x =  PYmat[, 1], y = PYmat[, 2], color = Cluster)) +
                    geom_point(size = .3) + ggform + xlab('Dimension 1') + 
                    ylab('Dimension 2') + ggtitle('LLE Clusters') +
                    theme(legend.key.size = unit(.25, 'cm'),
                          legend.key.height = unit(.25, 'cm'),
                          legend.key.width = unit(.25, 'cm'),
                          legend.title = element_text(size=8),
                          legend.text = element_text(size=6)) + guides(fill=guide_legend(ncol=2))

PrecipLEmanifolds <- ggplot(data = data.frame(PLEout, PLEclustout), aes(x = PLEout[, 1], y = PLEout[, 2], color = Cluster)) +
                     geom_point(size = .3) + ggform + xlab('Manifold 1') + 
                     ylab('Manifold 2') + ggtitle('LE Clusters') +
                     theme(legend.key.size = unit(.25, 'cm'),
                           legend.key.height = unit(.25, 'cm'),
                           legend.key.width = unit(.25, 'cm'),
                           legend.title = element_text(size=8),
                           legend.text = element_text(size=6)) 

ggarrange(PrecipSVDPCplot, PrecipKPCAPCplot, PrecipLLEdimplot, PrecipLEmanifolds, ncol = 2, nrow = 2)
```

```{r cleanup2, include = FALSE}
remove(PrecipSVD, rand2decomp, Pkpca2, Pkpcarand2, Plleres, PLE, MonthlyPrecip, rand2)
```

# Methods Considered

## Baseline: Persistence

```{r pers_mod, echo=FALSE, message=FALSE, eval=FALSE}
# Persistence Predictions (pred_(t + tau) = pred_t) ##############


# Need to define how far out we want tau to be
# Project document suggests 6 months?

## I based this on the following tutorial:
# https://machinelearningmastery.com/persistence-time-series-forecasting-with-python/


## Step 1
## transform univariate dataset into supervised learning problem
## load the data set and create a lagged representation
## i.e. given the observation at t predict the observation at t + tau
predprecip <- function(df, tau){
  ## df is dataframe
  ## tau is distance from prediction point
  mat <- as.matrix(df)
  predprecip <- matrix(NA, nrow = nrow(mat), ncol = ncol(mat) + tau)
  colnames(predprecip) <- c(colnames(mat), c(paste("x", 1:tau, sep = "")))
  for(i in 1:ncol(mat)){
    if(i < 3){
      predprecip[, i] <- mat[, i]
    } else{
      ttau <- i + tau
      predprecip[, ttau] <- mat[, i]
    }
  }
  preddf <- as.data.frame(predprecip)
  return(preddf)
}

## predicted precipitations when tau = 6 months
preddf <- predprecip(Pdat_df, 6)

## Step 2
## establish train and test datasets for test harness
## This isn't necessary, but if we want to add it we can

## Step 3
## Define persistence model
## This step is also unnecessary because we can extract it from step 1

## Step 4
## Make forecast and establish baseline performance
testmse <- function(Pdat_df, preddf){
  longdatorig <- gather(Pdat_df, time, precip, 3:ncol(Pdat_df))
  longdatpred <- gather(preddf, time, precip, 3:ncol(preddf))
  
  y <- data.frame("ytest" = longdatorig$precip, "yhat" = longdatpred[1:nrow(longdatorig), ]$precip)
  y <- na.omit(y)
  testmse <- mean((y$yhat - y$ytest)^2)
  return(testmse)
}

```

```{r pers_mod_mse, echo=FALSE, cache=TRUE}
load("pers_mse.RData")
## overall
# preddf <- predprecip(Pdat_df, 6)
# testmsepersall <- testmse(Pdat_df, preddf)
testmsepersall <- testmse

## Persistence MSE for training/testing
## predicted precipitations when tau = 6 months and limited to predicting 2017-2018
# jul16col <- which(colnames(Pdat_df) == "Jul2016")
# preddf <- predprecip(Pdat_df[, c(1:2, jul16col:ncol(Pdat_df))], 6)
## Persistence MSE for 2017-2018
# testmsepers <- testmse(Pdat_df[, c(1:2, jul16col:ncol(Pdat_df))], preddf)
testmsepers <- testmse_pers

```

The persistence model predicts that the precipitation for a location on a given date will be equal to the recorded precipitation at the same location $\tau$ months previously. For this baseline model, it seems that test MSE increases as $\tau$ approaches 6 month intervals away from the predicted date and decreases as $\tau$ approaches 12 month intervals away from the predicted date. These results make sense because this would mean that the error is highest when using weather data from the opposite time of year seasonally for prediction, and the error is lowest when using data from the same time of year seasonally for prediction. When $\tau = 6$ and all data are accounted for, the test mse was `r round(testmsepersall, 2)`. For the same value of $\tau$ when the data is limited to the testing set of data from 2017--2018 the test mse was `r round(testmsepers, 2)`. Although this MSE value is not as good as some of the others evident in the plot below, this is the value we'll be using for comparison because it matches the testing/training data set up and the $\tau$ values -- when applicable -- to our other models, so this is most suitable for comparison.

```{r pers_mod_plot, echo=FALSE, cache=TRUE}
## It seems MSE increases as tau approaches the 6 months mark and decreases 
## as it approaches the 12 month mark
# n <- 24
# msemat <- matrix(c(1:n, rep(NA, n)), nrow = n, ncol = 2, byrow = FALSE)
# colnames(msemat) <- c("tau", "mse")
# for(i in 1:n){
#   preddf <- predprecip(Pdat_df, i)
#   msemat[i, 2] <- testmse(Pdat_df, preddf)
# }
## above saved in this file
load("pers_msemat.RData")

ggplot(data=as.data.frame(msemat), aes(x=tau, y=mse, group=1)) +
  geom_line()+
  geom_point() +
  xlab("tau: Number of Months Previous to Prediction") +
  ylab("Test MSE") +
  labs(title = "Test MSE for Varying Values of tau")

```


## Baseline: Climatology

It is well known by experts in the field that one of the most simple models - the climatology model - performs relatively well given the ease of calculation and interpretation. The climatology model simply takes historical average for a given month and uses that as the prediction for that month in the future. Two climatology models were built with this data: one using the continuous precipitation values and one using the categorical precipitation values. The MSE of the model with the continuous values was 1.494, whereas the accuracy for the categorical values was 0.712. This values would need to be improved by other models described below to be considered as a replacement.

```{r clim_mod, cache=TRUE, include=FALSE}
# can't set this to eval = FALSE because need MSE for table
# Climatology Predictions (average of all previous months) ##############

# All months through 2016
Pdat_df_train = Pdat_df[,1:grep(pattern = "Dec2016", x = names(Pdat_df))]

# All months in 2017 only
Pdat_df_test = Pdat_df[, c(grep(pattern = "2017|2018", x = names(Pdat_df)))]
Pdat_df_test = cbind(Pdat_df[,c("long", "lat")], Pdat_df_test)

# Predict using climatology method (average of all previous months)
Pdat_df_preds = data.frame(Pdat_df_train[,c("long", "lat")])
predColNames = c(paste0(month.abb, "2017pred"), paste0(month.abb[1:2], "2018pred"))
for(i in 1:length(predColNames)) {
  Pdat_df_preds[, predColNames[i]] =
    rowMeans(Pdat_df_train[, grep(pattern = substr(predColNames[i], 1, 3),
                                  x = names(Pdat_df_train))])
}

# MSE from climatology predictions (average of all previous months)
MSE_clim = vector()
for(i in 1:length(predColNames)) {
  MSE_clim[i] = colMeans(((Pdat_df_test[grep(pattern = substr(predColNames[i], 1, 7),
                                        x = names(Pdat_df_test))] -
                        Pdat_df_preds[, grep(pattern = substr(predColNames[i], 1, 7),
                                             x = names(Pdat_df_preds))]) ^ 2),
                    na.rm = TRUE)
}

MSE_clim_avg = mean(MSE_clim)

# # Plot of the January predictions
# Jan2017Pred_raster <- rasterFromXYZ(Pdat_df_preds[, c("long", "lat", "Jan2017pred")])
# plot(Jan2017Pred_raster)
# 
# # Plot of the January true values
# Jan2017True_raster <- rasterFromXYZ(Pdat_df_test[, c("long", "lat", "Jan2017")])
# plot(Jan2017True_raster)

```



```{r clim_mod_plot, echo=FALSE, eval = FALSE}
state_map = map_data("state")
bmap = map_data("state")


# Fancy plots
ggplot() +
  coord_fixed(ratio = 1) +
  geom_raster(data = Pdat_df_preds,
              aes(x = long, y = lat, fill = May2017pred),
              alpha = 1) +
  geom_polygon(
    data = bmap,
    aes(x = long, y = lat, group = group),
    inherit.aes = F,
    colour = 'black',
    fill = NA,
    lwd = 0.5
  ) +
  scale_fill_gradientn(
    na.value = "white",
    # Limits need to be updated accordingly
    limits = c(min(Pdat_df_preds$May2017pred) - 0.5, 
               max(Pdat_df_preds$May2017pred) + 0.5),
    colours = c("blue", "green", "orange", "yellow")
  )+
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black",
                                fill = NA,
                                size = 0.5),
    panel.background = element_blank()
  ) + 
  labs(fill = "Precipication \nUnits", 
       title = "May 2017 Predicted Precipitation")

```

## Random Forest

```{r rf1, echo=FALSE, cache=TRUE, eval=FALSE}
# load 
load("SST_clustwide.RData")
load("Pdat_clustwide.RData")
load("SST_longagg.RData")
load("Pdat_longagg.RData")

## row 829 is beginning of 2017
train <- c(1:828)

all_train <- data.frame(Pdat_clustwide[train,], SST_clustwide[train,])
all_long_train <- gather(all_train, landclus, landclusavg, 2:13)
all_long_train <- all_long_train[-2]
all_test <- data.frame(Pdat_clustwide[-train,], SST_clustwide[-train,])
all_long_test <- gather(all_test, landclus, landclusavg, 2:13)
all_long_test <- all_long_test[-2]

## using sst and precip
set.seed(125498)
m <- round(sqrt(ncol(all_long_train) - 2), 0)
rf.sst <- randomForest(landclusavg ~ SeaCluster1 + SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5,
                       data = all_long_train,
                       mtry = m,
                       importance = TRUE)

```

```{r rf2, echo=FALSE, cache=TRUE}
# rf.sst$importance
## uncomment if want importance values and plots
# yhat.rf <- predict(rf.sst, newdata = as.matrix(all_long_test[,c(-1, -7)]))

load("all_precip_yyhat.RData")
testmserfcont <- mean((all_precip_yyhat$yhat - all_precip_yyhat$Precipitation)^2)


# plot(yhat.rf, all_long_test[, 8])
# abline(0,1)

```

```{r rf3, echo=FALSE, cache=TRUE}
load("all_yyhat_lag.RData")
testmse_rflag <- mean((all_yyhat_lag$yhat - all_yyhat_lag$Precipitation)^2)

```

There were two random forest models built: One with the standard data and the other with lagged data. For the standard model, the averages of the SST KPCA clusters for given dates were used to predict the averages of the precipitation KPCA clusters for the same dates (SST was categorized into 5 regions and precipitation was categorized into 12 regions). For the lagged model, the averages of the SST KPCA clusters for given dates were used to predict the averages of the precipitation KPCA clusters 6 months after a given date. Once the predictions were made, these predictions based on the cluster averages would then be assigned to the corresponding date and location of the larger original data set (i.e., individual observations instead of the averages) so that test MSE could be calculated in the context of individual observations instead of averages. 

For both models, the training set consisted of all observations before 2017, and the testing set consisted of the observations from 2017--2018. The number of variables randomly sampled as candidates at each split was set to 2 for both models. For both models, all of the predictors have about equal importance. The values are so close that it seems all of the SST cluster averages contribute, which makes sense. The test MSE for the standard model was `r round(testmserfcont, 2)`, and the test MSE for the model with the lagged data was `r round(testmse_rflag, 2)`.


## Ridge Regression

```{r ridge1, echo=FALSE, cache=TRUE}
# load 
load("SST_clustwide.RData")
load("Pdat_clustwide.RData")
load("SST_longagg.RData")
load("Pdat_longagg.RData")

## row 829 is beginning of 2017
train <- c(1:828)

all_train <- data.frame(Pdat_clustwide[train,], SST_clustwide[train,])
all_long_train <- gather(all_train, landclus, landclusavg, 2:13)
all_long_train <- all_long_train[-2]
all_test <- data.frame(Pdat_clustwide[-train,], SST_clustwide[-train,])
all_long_test <- gather(all_test, landclus, landclusavg, 2:13)
all_long_test <- all_long_test[-2]

## using sst and precip
set.seed(125498)
m <- round(sqrt(ncol(all_long_train) - 2), 0)
rf.sst <- randomForest(landclusavg ~ SeaCluster1 + SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5,
                       data = all_long_train,
                       mtry = m,
                       importance = TRUE)



##use cross validation to choose the tuning parameter
## actual data is too big so going to use a subset
set.seed(597546)
subset <- sample(1:nrow(all_long_train), 0.4*nrow(all_long_train))
x <- model.matrix(landclusavg ~ ., all_long_train[subset, ])[, -1]
y <- all_long_train[subset, ]$landclusavg

set.seed(897521)
train <- sample(1:nrow(x), 0.5*nrow(x))
test <- (-train)
y_test <- y[test]

grid <- 10^seq(10, -2, length = 100)
ridge_mod <- glmnet(x[train, ], y[train], alpha = 0,
                    lambda = grid, thresh = 1e-12)

set.seed(12598)
cv_out <- cv.glmnet(x[train, ], y[train], alpha = 0)
#plot(cv_out)
bestlam <- cv_out$lambda.min

```

```{r ridge2, echo=FALSE, cache=TRUE}

ridge_pred <- predict(ridge_mod, s = bestlam,
                      newx = x[test, ])

test_mse_ridge <- mean((ridge_pred - y_test)^2)

```


## Lasso Regression

```{r lasso1, echo=FALSE, cache=TRUE}

set.seed(1)
grid <- 10^seq(10, -2, length = 100)
lasso_mod <- glmnet(x[train, ], y[train], alpha = 1,
                    lambda = grid, thresh = 1e-12)

set.seed(12598)
cv_out <- cv.glmnet(x[train, ], y[train], alpha = 1)

bestlam <- cv_out$lambda.min



```

```{r lasso2, echo=FALSE, cache=TRUE}

lasso_pred <- predict(lasso_mod, s = bestlam,
                      newx = x[test, ])

test_mse_lasso <- mean((lasso_pred - y_test)^2)

```


## Principal Component Regression (PCR)

```{r pcr1, echo=FALSE, cache=TRUE}

pcr.fit <- pcr(landclusavg ~ SeaCluster1 + SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5,
               data = all_long_train,
               scale = TRUE,
               validation = "CV")



#validationplot(pcr.fit , val.type = "MSEP")



```

```{r pcr2, echo=FALSE, cache=TRUE}
#the lowest cross validation error at M=2
pcr.pred <- predict(pcr.fit, newdata = (all_long_test[,c(-1, -7)]), ncomp = 2)

testmse_PCRcont <- mean((pcr.pred - all_long_test[, 8])^2)
```


## Partial Least Squares (PLS)

```{r pls1, echo=FALSE, cache=TRUE}

pls.fit <- plsr(landclusavg ~ SeaCluster1 + SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5,
                data = all_long_train,
                scale = TRUE,
                validation = "CV")

#summary(pls.fit)

#validationplot(pls.fit , val.type = "MSEP")

```

```{r pls2, echo=FALSE, cache=TRUE}
#the lowest cross validation error at M=1
pls.pred <- predict(pls.fit, newdata = (all_long_test[,c(-1, -7)]), ncomp = 1)

testmse_PLScont <- mean((pls.pred - all_long_test[, 8])^2)

```



## GAM Regression on Continuous Data

```{r gam, echo=FALSE, cache=TRUE, eval=FALSE}
## BenREADME: I turned this off for now because I am not sure where the training data is coming from.
gam.1<- gam(Precipitation~s(SeaCluster1,2) +SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5, data = train)

```

The generalized linear approach proved to be less than ideal. The idea of approaching the data using GAM was the nonlinear relationships between the individual clusters and the precipitation data would be modeled while maintaining the additive nature of a linear model. Using the KPCA regions with continuous data as the response, we predicted the precipitation at a given time $\tau$ given the SST clustered data at the same time $\tau$. I began by running the GAM model without adjusting for nonlinearity for each SST cluster, to find that all of the variables were significant linearly, though barely so for SeaCluster3. Knowing this, I wanted to experiment with the nonlinearity between some of the clusters and the response, therefore I opted for smoothing splines on the first two SST clusters with degrees of freedom 2. SeaCluster1 and SeaCluster2 both proved to be significant at with 2 degrees of freedom. 

The GAM model proved to predict precipitation levels using SST data with very low accuracy. Against the 2017/18 test data, the best GAM model obtained an MSE of 8.5, with a smoothing spline on the first cluster. The following plot models the nonlinear relationship between the first SST cluster and the precipitation:

![](SST1.png)


## Convolutional Neural Networks

The essence of this research is to find a way to use SST at some time point to predict the precipitation in the United States at another (possibly different) time point. When looking back at the SST plots in the exploration section, we can see that the data follows the pattern of an image. In this case, the longitude would be substituted in the X axis, the the latitude would be substituted in the Y axis, and the "channel" feature would simply be the observed SST value. One thing to note is that the SST data was shrunk to eliminate the area with missing values. In this case, using all of the data, we have 842 "images" which correspond to different month and year combinations through the domain of the data. Taking into account this representation of the data, we can now think of this as an image processing problem of which we know how to handle using methods learned in class. 

One such method is convolutional neural networks (CNN) which are known to perform well on image-type data in both regression and classification contexts. The following subsections will describe the four best models from a selection of fifteen total models considered. Also note that, as in previous sections, the training data is contained up through 2016 and the test data includes 2017 and the two months in 2018.

In all, five different categories of models were considered, each one containing three models: 

  - Predicting precipitation at time $\tau$ from SST at time $\tau$
  - Predicting precipitation at time $\tau + 6$ from SST at time $\tau$ (6 month lag)
  - Predicting precipitation at time $\tau$ from SST principal components at time $\tau$

The five different categories of models vary aspects of the precipitation data: 

  - Naive precipitation regions with continuous data
  - Naive precipitation regions with basic categorical data
  - Naive precipitation regions with seasonal categorical data
  - KPCA precipitation regions with continuous data
  - KPCA precipitation regions with basic categorical data


#### Predict Average Temperature From Naive Regions  \newline


The original dimension of the precipitation data was too large for the CNN use case in this context, so four naive regions were created across the US based simply on geographical location. The simple average of these regions were then taken for each month and year to be predicted by the CNN. A visual representation of the four regions are shown in the plot below and are based off of a latitude of 38 and a longitude of -110.


```{r, include = TRUE, out.width = "75%", fig.align='center', echo = FALSE}

state_map = map_data("state")
bmap = map_data("state")

ggplot() +
  coord_fixed(ratio = 1) +
  geom_hline(yintercept = 38, lty = "dashed", color = "blue", size = 1) + 
  geom_vline(xintercept = -95, lty = "dashed", color = "blue", size = 1) + 
  annotate("text", x = -110, y = 45, label = "NW", size = 20, color = "blue") + 
  annotate("text", x = -80, y = 45, label = "NE", size = 20, color = "blue") + 
  annotate("text", x = -110, y = 30, label = "SW", size = 20, color = "blue") + 
  annotate("text", x = -80, y = 30, label = "SE", size = 20, color = "blue") + 
  geom_polygon(
    data = bmap,
    aes(x = long, y = lat, group = group),
    inherit.aes = F,
    colour = 'black',
    fill = NA,
    lwd = 0.5
  ) + 
  labs(fill = "", x = "Longitude", y = "Latitude", title = "Naive Regions") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black",
                                fill = NA,
                                size = 0.5),
    panel.background = element_blank()
  )

```


The first CNN was able to be trained now that the data were formatted accordingly. The CNN that performed best in this case contained a single convolution layer, a flatten, layer, 2 dense layers, and an output layer. Batch normalization was used between the dense layers and MSE was used as the loss function. A detailed summary is shown in the table below. This model was able to predict the averages of the four clusters with and MSE of 0.726. When expanding the predictions back to the original data set the MSE increased (as expected) to 3.01.


```{r include=TRUE, echo = FALSE}

kable(tibble(
  Options = c(
    "Convolution Layer",
    "Flatten",
    "Batch Normalization",
    "Dense Layer",
    "Batch Normalization",
    "Dense Layer",
    "Batch Normalization",
    "Dense Layer",
    "Epochs",
    "Batch Size",
    "Optimizer"
  ),
  `Model` = c(
    "filters = 32; kernal size = 5; activation = ReLU",
    "---",
    "---",
    "units = 800; activation = ReLU",
    "---",
    "units = 400; activation = ReLU",
    "---",
    "units = 4; activation = ReLU",
    10,
    32,
    "Adam; learning rate = 0.01"
  )
))

```


#### Predict Modal Temperature Category From Naive Regions with 6-Month Lag  \newline


The precipitation data can also be described in a categorical format representing low, normal, or high values. The normal category includes the 25th - 75th quantiles of data for the particular year and the low and high categories follow either as precipitation below the 25th quantile or above the 75th quantile respectively. The categorical data were then mapped to the four naive regions accordingly for each month and year. A 6-month lag was finally applied to the data such that the SST value at time $\tau$ predicts the precipitation at time $\tau + 6$ and the modal category for each region was calculated for prediction.

The best CNN found contains a convolutional layer followed by a series of dropout, flattening, and dense layers described in more detail in the table below. Also note that based on the way the data were formatted, sparse categorical crossentropy was used for the loss function. The accuracy of the model when predicting the category for each cluster was 0.713, and decreased (as expected) to 0.553 when expanding the predictions back to the full data set.


```{r include=TRUE, echo = FALSE}

kable(tibble(
  Options = c(
    "Convolution Layer",
    "Dropout",
    "Flatten",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Epochs",
    "Batch Size",
    "Optimizer"
  ),
  `Model` = c(
    "filters = 32; kernal size = 5; activation = ReLU",
    "10%",
    "---",
    "units = 100; activation = ReLU",
    "30%",
    "units = 50; activation = ReLU",
    "30%",
    "units = 25; activation = ReLU",
    "40%",
    "units = 4; activation = ReLU",
    30,
    32,
    "Adam; learning rate = 0.01"
  )
))
  
  
```




#### Predict Average Temperature From KPCA Regions  \newline


Based on the results achieved in the previous sections with the naive regions, we wanted to explore more methodical ways of creating temperature regions. One such way emerged from the results of the KPCA analysis done on the temperature data from a previous region. Using those results, the temperature data were separated into twelve distinct regions displayed previously in the KPCA section.

```{r, include = FALSE, out.width = "75%", fig.align='center', echo = FALSE}

ggplot() +
  coord_fixed(ratio = 1) +
  geom_raster(data = precip_region_cont_df_wide, aes(
    x = long,
    y = lat,
    fill = Cluster
  ), alpha = 1) +
  geom_polygon(
    data = bmap,
    aes(x = long, y = lat, group = group),
    inherit.aes = F,
    colour = 'black',
    fill = NA,
    lwd = 0.5
  ) + 
  labs(fill = "Region", x = "Longitude", y = "Latitude", title = "KPCA Regions") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black",
                                fill = NA,
                                size = 0.5),
    panel.background = element_blank()
  )


```


The average precipitation for each month and year was calculated to be predicted by the CNN. The best model for this case included a convolutional layer followed by a series of dense and dropout layers shown in detail in the table below. MSE was used as the loss function as in the naive region case. The MSE for predicting the region averages was 2.712 which decreased slightly to 2.47 when expanded to the full data set.



```{r include=TRUE, echo = FALSE}

kable(tibble(
  Options = c(
    "Convolution Layer",
    "Flatten",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Epochs",
    "Batch Size",
    "Optimizer"
  ),
  `Model` = c(
    "filters = 32; kernal size = 5; activation = ReLU",
    "---",
    "units = 100; activation = ReLU",
    "20%",
    "units = 50; activation = ReLU",
    "20%",
    "units = 25; activation = ReLU",
    "20%",
    "units = 12; activation = ReLU",
    100,
    32,
    "Adam; learning rate = 0.001"
  )
))

```




#### Predict Modal Temperature Category From KPCA Regions  \newline


The temperature regions determined by PCA can also be mapped to different temperature categories as described in an earlier section with the naive clusters. The data were grouped based on these twelve regions and three temperature categories (low, normal, high) and the most frequent temperature category for each group was taken to be the value representative of that region (similar to the temperature average approach with the continuous data). The goal was now to simply predict the temperature category for each of the 12 regions.

As with the naive region case, each of the twelve clusters were trained on the CNN model separately. The optimal model for this case included a convolution layer, flattening, batch normalization, dropout, and a few dense layers. Also note that sparse categorical crossentropy was used for the loss function. The accuracy of predicting the categories for each cluster was 0.565 which decreased to 0.347 when the predictions expanded to the full data.


```{r include=TRUE, echo = FALSE}

kable(tibble(
  Options = c(
    "Convolution Layer",
    "Flatten",
    "Batch Normalization",
    "Dense Layer",
    "Dropout",
    "Batch Normalization",
    "Dense Layer",
    "Dense Layer",
    "Epochs",
    "Batch Size",
    "Optimizer"
  ),
  `Model` = c(
    "filters = 32; kernal size = 5; activation = ReLU",
    "---",
    "---",
    "units = 800; activation = ReLU",
    "20%",
    "---",
    "units = 400; activation = ReLU",
    "units = 4; activation = ReLU",
    10,
    32,
    "Adam; learning rate = 0.01; decay = 0.01"
  )
))


```





# Results

```{r echo=FALSE, results='asis'}
MSE_CNN_naive_cont = 3.01
MSE_KPCA_cont = 2.47

df <- data.frame("Model" = c("Baseline: Climatology",
                             "Baseline: Persistence",
                             "Random Forest",
                             "Random Forest, Lagged Data",
                             "Ridge Regression",
                             "Lasso Regression",
                             "CNN: Naive Regions, Continuous Temp",
                             "CNN: KPCA Regions, Continuous Temp"),
                 "Test MSE" = c(MSE_clim_avg, testmsepers, testmserfcont,
                                testmse_rflag,
                                test_mse_ridge, test_mse_lasso,
                                MSE_CNN_naive_cont, MSE_KPCA_cont))

print(xtable(df, caption = "Test MSE Values"), include.rownames = FALSE)
```


```{r echo=FALSE, results='asis'}
Accuracy_climatology = 0.712
Accuracy_naive_cat_lag = 0.553
Accuracy_KPCA_cat = 0.347

df2 <- data.frame("Model" = c("Baseline: Climatology",
                              "CNN: Naive Regions, Categorical Temp",
                              "CNN: KPCA Regions, Categorical Temp"),
                  "Test Accuracy" = c(Accuracy_climatology, 
                                      Accuracy_naive_cat_lag, Accuracy_KPCA_cat))

print(xtable(df2, caption = "Test Accuracy Values"), include.rownames = FALSE)
```

# Conclusion


One issue that arose in the CCN analysis using the categorical precipitation was that the majority of locations have "normal" precipitation levels. Even when considering different class weights to account for the differences, it was difficult for the model to accurately predict the true categories. Future analysis could attempt to improve these results by investigating different values of $\tau$ in the lag models, devising a different methodology for categorizing precipitation, **ANYTHING ELSE??**




