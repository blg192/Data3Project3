---
title:  |
  | \vspace{5cm} Stat 8330 Final Project Report
author: "Katie Price, Emily Scully, Ben Graves, Ellen Fitzsimmons, and Mira Isnainy"
date: \today
output: pdf_document
indent: true
header-includes:
  - \usepackage{pdfpages}
  - \usepackage{indentfirst}
---

\newpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("ncdf4")
#install.packages("raster")
#install.packages("rgdal")
#install.packages("ggplot2")
#install.packages("ncdf4.helpers")
#install.packages("PCICt")
library(ncdf4) # package for netcdf manipulation
library(raster) # package for raster manipulation
library(rgdal) # package for geospatial analysis
library(ggplot2) # package for plotting
library(ncdf4.helpers)
library(PCICt)
library(lubridate)
library(dplyr)
library(maps)
library(fields)
library(tidyr)
library(glmnet)
library(randomForest)
library(BART)
library(xtable)
library(pls)
options(xtable.comment = FALSE)


library(knitr)
library(ggpubr)
library(tidyverse)
library(tibble)
library(tfdatasets)
library(keras)
library(reshape2)

load("precip_region_cont_df_wide.Rdata")

```

# Introduction
Meteorologists study long and hard to predict weather patterns, day in and day out. Anticipating precipitation in North America is not only a convenience for the general public, but a necessity for farmers and agriculturalists. The importance of clean water is a secret to no one. Clean water is not only a key factor in the survival of nearly all living species, but also a key resource for power and production in the modern world. From food production to electricity, water can be used in many different forms for many important aspects of the human experience. Predicting precipitation and subsequently predicting to some extent the frequency and volume of available freshwater is a valuable and essential analysis. This study looks to examine the relationship between sea surface temperatures (henceforth SST) patterns in the tropical Pacific Ocean and precipitation over North America. It was our desire to be able to predict precipitation levels based on SST, both contemporaneously and on a future-time basis. We used several different methods to predict the precipitation outcomes as accurately as possible, including: **insert all methods here**. 

# Exploratory Data Analysis
The data available includes  monthly SST anomalies on a grid for the period January 1948 through February 2018. These "anomalies" consist of differences of the temperature from a long-term average. Additionally, the data includes precipitation data for a majority of the North American continent.

# Methods Considered

```{r data, echo=FALSE}
## import data
# To load the RData
load("data_in_dfs.RData")

```




## Baseline: Persistence
- $\tau$ represents number of months before predicted precipitation date



## Random Forest

- For random forest, predicted precip from sst
- Ben should add his stuff about PCA and clust before this because it's based on that
- used kpca clustering for land and sea and got average precip and avg sst for their respective clusters (5 clusters for sst and 12 clusters for precip)
- used average sst in given month-year for each sea cluster as predictors of the precipitation of the same month-year
- training set is time pre 2017 and testing set is 2017 and 2018
- The number of variables randomly sampled as candidates at each split is set to 2



## Baseline: Climatology

It is well known by experts in the field that one of the most simple models - the climatology model - performs relatively well given the ease of calculation and interpretation. The climatology model simply takes historical average for a given month and uses that as the prediction for that month in the future. Two climatology models were built with this data: one using the continuous precipitation values and one using the categorical precipitation values. The MSE of the model with the continuous values was 1.494, whereas the accuracy for the categorical values was 0.712. This values would need to be improved by other models described below to be considered as a replacement.

```{r clim_mod, cache=TRUE, include=FALSE}
# can't set this to eval = FALSE because need MSE for table
# Climatology Predictions (average of all previous months) ##############

# All months through 2016
Pdat_df_train = Pdat_df[,1:grep(pattern = "Dec2016", x = names(Pdat_df))]

# All months in 2017 only
Pdat_df_test = Pdat_df[, c(grep(pattern = "2017|2018", x = names(Pdat_df)))]
Pdat_df_test = cbind(Pdat_df[,c("long", "lat")], Pdat_df_test)

# Predict using climatology method (average of all previous months)
Pdat_df_preds = data.frame(Pdat_df_train[,c("long", "lat")])
predColNames = c(paste0(month.abb, "2017pred"), paste0(month.abb[1:2], "2018pred"))
for(i in 1:length(predColNames)) {
  Pdat_df_preds[, predColNames[i]] =
    rowMeans(Pdat_df_train[, grep(pattern = substr(predColNames[i], 1, 3),
                                  x = names(Pdat_df_train))])
}

# MSE from climatology predictions (average of all previous months)
MSE_clim = vector()
for(i in 1:length(predColNames)) {
  MSE_clim[i] = colMeans(((Pdat_df_test[grep(pattern = substr(predColNames[i], 1, 7),
                                        x = names(Pdat_df_test))] -
                        Pdat_df_preds[, grep(pattern = substr(predColNames[i], 1, 7),
                                             x = names(Pdat_df_preds))]) ^ 2),
                    na.rm = TRUE)
}

MSE_clim_avg = mean(MSE_clim)

# # Plot of the January predictions
# Jan2017Pred_raster <- rasterFromXYZ(Pdat_df_preds[, c("long", "lat", "Jan2017pred")])
# plot(Jan2017Pred_raster)
# 
# # Plot of the January true values
# Jan2017True_raster <- rasterFromXYZ(Pdat_df_test[, c("long", "lat", "Jan2017")])
# plot(Jan2017True_raster)

```



```{r clim_mod_plot, echo=FALSE, eval = FALSE}
state_map = map_data("state")
bmap = map_data("state")


# Fancy plots
ggplot() +
  coord_fixed(ratio = 1) +
  geom_raster(data = Pdat_df_preds,
              aes(x = long, y = lat, fill = May2017pred),
              alpha = 1) +
  geom_polygon(
    data = bmap,
    aes(x = long, y = lat, group = group),
    inherit.aes = F,
    colour = 'black',
    fill = NA,
    lwd = 0.5
  ) +
  scale_fill_gradientn(
    na.value = "white",
    # Limits need to be updated accordingly
    limits = c(min(Pdat_df_preds$May2017pred) - 0.5, 
               max(Pdat_df_preds$May2017pred) + 0.5),
    colours = c("blue", "green", "orange", "yellow")
  )+
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black",
                                fill = NA,
                                size = 0.5),
    panel.background = element_blank()
  ) + 
  labs(fill = "Precipication \nUnits", 
       title = "May 2017 Predicted Precipitation")

```

## Baseline: Persistence

- test MSE increases as tau approaches 6 month intervals away from the predicted date and decreases as tau approaches 12 month intervals away from the predicted date
- Basically MSE is lower when using data from the same time of year as the predicted date

```{r pers_mod, echo=FALSE, message=FALSE}
# Persistence Predictions (pred_(t + tau) = pred_t) ##############


# Need to define how far out we want tau to be
# Project document suggests 6 months?

## I based this on the following tutorial:
# https://machinelearningmastery.com/persistence-time-series-forecasting-with-python/


## Step 1
## transform univariate dataset into supervised learning problem
## load the data set and create a lagged representation
## i.e. given the observation at t predict the observation at t + tau
predprecip <- function(df, tau){
  ## df is dataframe
  ## tau is distance from prediction point
  mat <- as.matrix(df)
  predprecip <- matrix(NA, nrow = nrow(mat), ncol = ncol(mat) + tau)
  colnames(predprecip) <- c(colnames(mat), c(paste("x", 1:tau, sep = "")))
  for(i in 1:ncol(mat)){
    if(i < 3){
      predprecip[, i] <- mat[, i]
    } else{
      ttau <- i + tau
      predprecip[, ttau] <- mat[, i]
    }
  }
  preddf <- as.data.frame(predprecip)
  return(preddf)
}

## predicted precipitations when tau = 6 months
preddf <- predprecip(Pdat_df, 6)

## Step 2
## establish train and test datasets for test harness
## This isn't necessary, but if we want to add it we can

## Step 3
## Define persistence model
## This step is also unnecessary because we can extract it from step 1

## Step 4
## Make forecast and establish baseline performance
testmse <- function(Pdat_df, preddf){
  longdatorig <- gather(Pdat_df, time, precip, 3:ncol(Pdat_df))
  longdatpred <- gather(preddf, time, precip, 3:ncol(preddf))
  
  y <- data.frame("ytest" = longdatorig$precip, "yhat" = longdatpred[1:nrow(longdatorig), ]$precip)
  y <- na.omit(y)
  testmse <- mean((y$yhat - y$ytest)^2)
  return(testmse)
}

```

```{r pers_mod_plot, echo=FALSE, cache=TRUE}
## It seems MSE increases as tau approaches the 6 months mark and decreases 
## as it approaches the 12 month mark
n <- 24
msemat <- matrix(c(1:n, rep(NA, n)), nrow = n, ncol = 2, byrow = FALSE)
colnames(msemat) <- c("tau", "mse")
for(i in 1:n){
  preddf <- predprecip(Pdat_df, i)
  msemat[i, 2] <- testmse(Pdat_df, preddf)
}

ggplot(data=as.data.frame(msemat), aes(x=tau, y=mse, group=1)) +
  geom_line()+
  geom_point() +
  xlab("tau: Number of Months Ahead of Prediction") +
  ylab("Test MSE") +
  labs(title = "Test MSE for Varying Values of tau")

```

```{r pers_mod_mse, echo=FALSE, cache=TRUE}
## Persistence MSE for training/testing
## predicted precipitations when tau = 6 months and limited to predicting 2017-2018
jul16col <- which(colnames(Pdat_df) == "Jul2016")
preddf <- predprecip(Pdat_df[, c(1:2, jul16col:ncol(Pdat_df))], 6)
## Persistence MSE for 2017-2018
testmsepers <- testmse(Pdat_df[, c(1:2, jul16col:ncol(Pdat_df))], preddf)

```


## Random Forest

```{r rf1, echo=FALSE, cache=TRUE, eval=FALSE}
# load 
load("SST_clustwide.RData")
load("Pdat_clustwide.RData")
load("SST_longagg.RData")
load("Pdat_longagg.RData")

## row 829 is beginning of 2017
train <- c(1:828)

all_train <- data.frame(Pdat_clustwide[train,], SST_clustwide[train,])
all_long_train <- gather(all_train, landclus, landclusavg, 2:13)
all_long_train <- all_long_train[-2]
all_test <- data.frame(Pdat_clustwide[-train,], SST_clustwide[-train,])
all_long_test <- gather(all_test, landclus, landclusavg, 2:13)
all_long_test <- all_long_test[-2]

## using sst and precip
set.seed(125498)
m <- round(sqrt(ncol(all_long_train) - 2), 0)
rf.sst <- randomForest(landclusavg ~ SeaCluster1 + SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5,
                       data = all_long_train,
                       mtry = m,
                       importance = TRUE)

```

```{r rf2, echo=FALSE, cache=TRUE}
# rf.sst$importance
## uncomment if want importance values and plots
# yhat.rf <- predict(rf.sst, newdata = as.matrix(all_long_test[,c(-1, -7)]))

load("all_precip_yyhat.RData")
testmserfcont <- mean((all_precip_yyhat$yhat - all_precip_yyhat$Precipitation)^2)

# plot(yhat.rf, all_long_test[, 8])
# abline(0,1)

```


## RIDGE REGRESSION

```{r ridge1, echo=FALSE, cache=TRUE}
# load 
load("SST_clustwide.RData")
load("Pdat_clustwide.RData")
load("SST_longagg.RData")
load("Pdat_longagg.RData")

## row 829 is beginning of 2017
train <- c(1:828)

all_train <- data.frame(Pdat_clustwide[train,], SST_clustwide[train,])
all_long_train <- gather(all_train, landclus, landclusavg, 2:13)
all_long_train <- all_long_train[-2]
all_test <- data.frame(Pdat_clustwide[-train,], SST_clustwide[-train,])
all_long_test <- gather(all_test, landclus, landclusavg, 2:13)
all_long_test <- all_long_test[-2]

## using sst and precip
set.seed(125498)
m <- round(sqrt(ncol(all_long_train) - 2), 0)
rf.sst <- randomForest(landclusavg ~ SeaCluster1 + SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5,
                       data = all_long_train,
                       mtry = m,
                       importance = TRUE)



##use cross validation to choose the tuning parameter
## actual data is too big so going to use a subset
set.seed(597546)
subset <- sample(1:nrow(all_long_train), 0.4*nrow(all_long_train))
x <- model.matrix(landclusavg ~ ., all_long_train[subset, ])[, -1]
y <- all_long_train[subset, ]$landclusavg

set.seed(897521)
train <- sample(1:nrow(x), 0.5*nrow(x))
test <- (-train)
y_test <- y[test]

grid <- 10^seq(10, -2, length = 100)
ridge_mod <- glmnet(x[train, ], y[train], alpha = 0,
                    lambda = grid, thresh = 1e-12)

set.seed(12598)
cv_out <- cv.glmnet(x[train, ], y[train], alpha = 0)
#plot(cv_out)
bestlam <- cv_out$lambda.min

```

```{r ridge2, echo=FALSE, cache=TRUE}

ridge_pred <- predict(ridge_mod, s = bestlam,
                      newx = x[test, ])

test_mse_ridge <- mean((ridge_pred - y_test)^2)
test_mse_ridge

```


## LASSO REGRESSION

```{r ridge1, echo=FALSE, cache=TRUE}

set.seed(1)
grid <- 10^seq(10, -2, length = 100)
lasso_mod <- glmnet(x[train, ], y[train], alpha = 1,
                    lambda = grid, thresh = 1e-12)

set.seed(12598)
cv_out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv_out)
bestlam <- cv_out$lambda.min
bestlam


```

```{r ridge2, echo=FALSE, cache=TRUE}

lasso_pred <- predict(lasso_mod, s = bestlam,
                      newx = x[test, ])

test_mse_lasso <- mean((lasso_pred - y_test)^2)

```


## PRINCIPAL COMPONENT REGRESSION (PCR)

```{r pcr1, echo=FALSE, cache=TRUE}

pcr.fit <- pcr(landclusavg ~ SeaCluster1 + SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5,
               data = all_long_train,
               scale = TRUE,
               validation = "CV")

summary(pcr.fit)

#validationplot(pcr.fit , val.type = "MSEP")



```

```{r pcr2, echo=FALSE, cache=TRUE}
#the lowest cross validation error at M=2
pcr.pred <- predict(pcr.fit, newdata = (all_long_test[,c(-1, -7)]), ncomp = 2)

testmse_PCRcont <- mean((pcr.pred - all_long_test[, 8])^2)
```


## PARTIAL LEAST SQUARE (PLS)

```{r pls1, echo=FALSE, cache=TRUE}

pls.fit <- plsr(landclusavg ~ SeaCluster1 + SeaCluster2 + SeaCluster3 + SeaCluster4 + SeaCluster5,
                data = all_long_train,
                scale = TRUE,
                validation = "CV")

#summary(pls.fit)

#validationplot(pls.fit , val.type = "MSEP")

```

```{r pls2, echo=FALSE, cache=TRUE}
#the lowest cross validation error at M=1
pls.pred <- predict(pls.fit, newdata = (all_long_test[,c(-1, -7)]), ncomp = 1)

testmse_PLScont <- mean((pls.pred - all_long_test[, 8])^2)

```


## Convolutional Neural Networks

The essence of this research is to find a way to use SST at some time point to predict the precipitation in the United States at another (possibly different) time point. When looking back at the SST plots in the exploration section, we can see that the data follows the pattern of an image. In this case, the longitude would be substituted in the X axis, the the latitude would be substituted in the Y axis, and the "channel" feature would simply be the observed SST value. One thing to note is that the SST data was shrunk to eliminate the area with missing values. In this case, using all of the data, we have 842 "images" which correspond to different month and year combinations through the domain of the data. Taking into account this representation of the data, we can now think of this as an image processing problem of which we know how to handle using methods learned in class. 

One such method is convolutional neural networks (CNN) which are known to perform well on image-type data in both regression and classification contexts. The following subsections will describe the four best models from a selection of fifteen total models considered. Also note that, as in previous sections, the training data is contained up through 2016 and the test data includes 2017 and the two months in 2018.

In, all five different categories of models were considered, each one containing three models: 

  - Predicting precipitation at time $\tau$ from SST at time $\tau$
  - Predicting precipitation at time $\tau + 6$ from SST at time $\tau$ (6 month lag)
  - Predicting precipitation at time $\tau$ from SST principal components at time $\tau$

The five different categories of models vary aspects of the precipitation data: 

  - Naive precipitation regions with continuous data
  - Naive precipitation regions with basic categorical data
  - Naive precipitation regions with seasonal categorical data
  - KPCA precipitation regions with continuous data
  - KPCA precipitation regions with basic categorical data


#### Predict Average Temperature From Naive Regions  \newline


The original dimension of the precipitation data was too large for the CNN use case in this context, so four naive regions were created across the US based simply on geographical location. The simple average of these regions were then taken for each month and year to be predicted by the CNN. A visual representation of the four regions are shown in the plot below and are based off of a latitude of 38 and a longitude of -110.


```{r, include = TRUE, out.width = "75%", fig.align='center', echo = FALSE}

state_map = map_data("state")
bmap = map_data("state")

ggplot() +
  coord_fixed(ratio = 1) +
  geom_hline(yintercept = 38, lty = "dashed", color = "blue", size = 1) + 
  geom_vline(xintercept = -95, lty = "dashed", color = "blue", size = 1) + 
  annotate("text", x = -110, y = 45, label = "NW", size = 20, color = "blue") + 
  annotate("text", x = -80, y = 45, label = "NE", size = 20, color = "blue") + 
  annotate("text", x = -110, y = 30, label = "SW", size = 20, color = "blue") + 
  annotate("text", x = -80, y = 30, label = "SE", size = 20, color = "blue") + 
  geom_polygon(
    data = bmap,
    aes(x = long, y = lat, group = group),
    inherit.aes = F,
    colour = 'black',
    fill = NA,
    lwd = 0.5
  ) + 
  labs(fill = "", x = "Longitude", y = "Latitude", title = "Naive Regions") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black",
                                fill = NA,
                                size = 0.5),
    panel.background = element_blank()
  )

```


The first CNN was able to be trained now that the data were formatted accordingly. The CNN that performed best in this case contained a single convolution layer, a flatten, layer, 2 dense layers, and an output layer. Batch normalization was used between the dense layers. MSE was used as the loss function. A detailed summary is shown in the table below. This model was able to predict the averages of the four clusters with and MSE of 0.726. When expanding the predictions back to the original data set the MSE increased (as expected) 3.01.


```{r include=TRUE, echo = FALSE}

kable(tibble(
  Options = c(
    "Convolution Layer",
    "Flatten",
    "Batch Normalization",
    "Dense Layer",
    "Batch Normalization",
    "Dense Layer",
    "Batch Normalization",
    "Dense Layer",
    "Epochs",
    "Batch Size",
    "Optimizer"
  ),
  `Model` = c(
    "filters = 32; kernal size = 5; activation = ReLU",
    "---",
    "---",
    "units = 800; activation = ReLU",
    "---",
    "units = 400; activation = ReLU",
    "---",
    "units = 4; activation = ReLU",
    10,
    32,
    "Adam; learning rate = 0.01"
  )
))

```


#### Predict Modal Temperature Category From Naive Regions with 6-Month Lag  \newline


The precipitation data can also described in a categorical format representing low, normal, or high values. The normal category includes the 25th - 75th quantiles of data for the particular year and the low and high categories follow either as precipitation below the 25th quantile or above the 75th quantile respectively. The categorical data were then mapped to the four naive regions accordingly for each month and year. A 6-month lag was finally applied to the data such that the SST value at time $\tau$ predicts the precipitation at time $\tau + 6$ and the modal category for each region was calculated for prediction.

The best CNN found contains a convolutional layer followed by a series of dropout, flattening, and dense layers described in more detail in the table below. Also note that based on the way the data were formatted, sparse categorical crossentropy was used for the loss function. The accuracy of the model when predicting the category for each cluster was 0.713, and decreased (as expected) to 0.553 when expanding the predictions back to the full data set.


```{r include=TRUE, echo = FALSE}

kable(tibble(
  Options = c(
    "Convolution Layer",
    "Dropout",
    "Flatten",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Epochs",
    "Batch Size",
    "Optimizer"
  ),
  `Model` = c(
    "filters = 32; kernal size = 5; activation = ReLU",
    "10%",
    "---",
    "units = 100; activation = ReLU",
    "30%",
    "units = 50; activation = ReLU",
    "30%",
    "units = 25; activation = ReLU",
    "40%",
    "units = 4; activation = ReLU",
    30,
    32,
    "Adam; learning rate = 0.01"
  )
))
  
  
```




#### Predict Average Temperature From KPCA Regions  \newline


Based on the results achieved in the previous sections with the naive regions, we wanted to explore more methodical ways of creating temperature regions. One such way emerged from the results of the KPCA analysis done on the temperature data from a previous region. Using those results, the temperature data were separated into twelve distinct regions displayed below:

```{r, include = TRUE, out.width = "75%", fig.align='center', echo = FALSE}

ggplot() +
  coord_fixed(ratio = 1) +
  geom_raster(data = precip_region_cont_df_wide, aes(
    x = long,
    y = lat,
    fill = Cluster
  ), alpha = 1) +
  geom_polygon(
    data = bmap,
    aes(x = long, y = lat, group = group),
    inherit.aes = F,
    colour = 'black',
    fill = NA,
    lwd = 0.5
  ) + 
  labs(fill = "Region", x = "Longitude", y = "Latitude", title = "KPCA Regions") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black",
                                fill = NA,
                                size = 0.5),
    panel.background = element_blank()
  )


```


The average precipitation for each month and year was calculated to be predicted by the CNN. The best model for this case included a convolutional layer followed by a series of dense and dropout layers shown in detail in the table below. MSE was used as the loss function as in the naive region case. The MSE for predicting the region averages was 2.712 which decreased slightly to 2.47 when expanded to the full data set.



```{r include=TRUE, echo = FALSE}

kable(tibble(
  Options = c(
    "Convolution Layer",
    "Flatten",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Dropout",
    "Dense Layer",
    "Epochs",
    "Batch Size",
    "Optimizer"
  ),
  `Model` = c(
    "filters = 32; kernal size = 5; activation = ReLU",
    "---",
    "units = 100; activation = ReLU",
    "20%",
    "units = 50; activation = ReLU",
    "20%",
    "units = 25; activation = ReLU",
    "20%",
    "units = 12; activation = ReLU",
    100,
    32,
    "Adam; learning rate = 0.001"
  )
))

```




#### Predict Modal Temperature Category From KPCA Regions  \newline


The temperature regions determined by PCA can also be mapped to different temperature categories as described in an earlier section with the naive clusters. The data were grouped based on these twelve regions and three temperature categories (low, normal, high) and the most frequent temperature category for each group was taken to be the value representative of that region (similar to the temperature average approach with the continuous data). The goal was now to simply predict the temperature category for each of the 12 regions.

As with the naive region case, each of the twelve clusters were trained on the CNN model separately. The optimal model for this case included a convolution layer, flattening, batch normalization, dropout, and a few dense layers. Also note that sparse categorical crossentropy was used for the loss function. The accuracy of predicting the categories for each cluster was 0.565 which decreased to 0.347 when the predictions expanded to the full data.


```{r include=TRUE, echo = FALSE}

kable(tibble(
  Options = c(
    "Convolution Layer",
    "Flatten",
    "Batch Normalization",
    "Dense Layer",
    "Dropout",
    "Batch Normalization",
    "Dense Layer",
    "Dense Layer",
    "Epochs",
    "Batch Size",
    "Optimizer"
  ),
  `Model` = c(
    "filters = 32; kernal size = 5; activation = ReLU",
    "---",
    "---",
    "units = 800; activation = ReLU",
    "20%",
    "---",
    "units = 400; activation = ReLU",
    "units = 4; activation = ReLU",
    10,
    32,
    "Adam; learning rate = 0.01; decay = 0.01"
  )
))


```





# Results

```{r echo=FALSE, results='asis'}
MSE_CNN_naive_cont = 3.01
MSE_KPCA_cont = 2.47

df <- data.frame("Model" = c("Baseline: Climatology",
                             "Baseline: Persistence",
                             "Random Forest",
                             "Ridge Regression",
                             "Lasso Regression",
                             "CNN: Naive Regions, Continuous Temp",
                             "CNN: KPCA Regions, Continuous Temp"),
                 "Test MSE" = c(MSE_clim_avg, testmsepers, testmserfcont,
                                test_mse_ridge, test_mse_lasso,
                                MSE_CNN_naive_cont, MSE_KPCA_cont))

print(xtable(df, caption = "Test MSE Values"), include.rownames = FALSE)
```


```{r echo=FALSE, results='asis'}
Accuracy_climatology = 0.712
Accuracy_naive_cat_lag = 0.553
Accuracy_KPCA_cat = 0.347

df2 <- data.frame("Model" = c("Baseline: Climatology",
                              "Baseline: Persistence",
                              "CNN: Naive Regions, Categorical Temp",
                              "CNN: KPCA Regions, Categorical Temp"),
                  "Test Accuracy" = c(Accuracy_climatology, testmsepers, 
                                      MSE_CNN_naive_cont, MSE_KPCA_cont))

print(xtable(df2, caption = "Test Accuracy Values"), include.rownames = FALSE)
```

## Conclusion


One issue that arose in the CCN analysis using the categorical precipitation was that the majority of locations have "normal" precipitation levels. Even when considering different class weights to account for the differences, it was difficult for the model to accurately predict the true categories. Future analysis could attempt to improve these results by investigating different values of $\tau$ in the lag models, devising a different methodology for categorizing precipitation, **ANYTHING ELSE??**
