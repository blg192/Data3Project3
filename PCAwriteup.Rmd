---
title: "PCA Section"
author: "Benjamin Graves"
date: \today
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xtable)
library(knitr)
library(readr)
library(ggplot2)
library(kernlab)
library(NMF)
library(lle)
library(dimRed)
library(loe)
library(RSpectra)
library(maps)
library(fields)
library(raster) 
library(NbClust)
library(reshape2)
library(maps)
library(fields)
library(ggpubr)
form <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
              panel.background = element_blank(), axis.line = element_line(colour = "black"))
load("data_in_dfs.RData") ## COMMENT OUT BEFORE COMBINE
```

\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\proglang=\textsf
\let\code=\texttt

## PCA Methods and Clustering

\ \ \ \ \ \ \ \ 
In order to decrease dimensionality of the data set, we conduct a series of dimension reduction techniques and then use these results to create location based clusters. For dimension reduction of both the SST and precipitation data, we consider principal components analysis using the singular value decomposition (SVD), kernel PCA (KPCA), local linear embedding (LLE), and Laplacian Eigenmaps (LE). The number of principal components for the two PCAs are selected by assessing cumulative variance accounted for, comparing to randomized data, dimensional minimization, and impact on clustering clarity. Other factors such as kernel smoothing method for the KPCA and number of neighbors for LLE are also optimized. 

\ \ \ \ \ \ \ \ 
Determination of the number of regional clusters for each of the four dimensional reduction methods above is done with k-means clustering and aided with the \pkg{NbClust} package. This package uses 30 different indices to suggest an appropriate number of clusters and varies all combinations of number of clusters, distance measures, and clustering methods. The final number of clusters is decided using a 'majority rule' approach and looking at cluster clarity and complexity. Basically, the number of clusters with the most indices in agreement were considered and then of those the one with the most visually clear clusters when plotted were chosen. 

\ \ \ \ \ \ \ \ 
Results from each of the dimensional reduction methods and clustering are presented below for the SST and precipitation data sets. 

### SST Results

```{r SSTSVDsetup, include=FALSE, cache=TRUE}
SST_df <- na.omit(SST_df)
SST_df <- apply(SST_df, 2, as.numeric)
LongLat <- SST_df[, 1:2]
MonthlySST <- SST_df[ , -c(1,2)]

## Center data
# SSTgrand <- mean(MonthlySST)
# MonthlySST_cent <- MonthlySST - SSTgrand



## Start with SVD to get an idea of Prin Comp needed
SSTSVD <- svd(MonthlySST)
SSTsingvals <- matrix(NA, nrow = 842, ncol = 2)
SSTsingvals[, 1] <- 1:842
SSTsingvals[, 2] <- SSTSVD$d
SVDvaracc <- cumsum(SSTSVD$d[1:33]^2)/sum(SSTSVD$d^2)*100


## Set up random data
set.seed(938837)
rand <- matrix(NA, 2512, 842)
for (i in 1:842) {
    rand[ , i] <- MonthlySST[sample(nrow(MonthlySST)), i]
}

randdecomp <- svd(rand)
SSTsingvals <- data.frame(SSTsingvals, as.numeric(randdecomp$d))
colnames(SSTsingvals) <- c('Component', 'Real Data Trace', 'Random Data Trace')

## PC DF
SSTUD <- SSTSVD$u %*% diag(SSTSVD$d)
SSTUD <- data.frame(LongLat, SSTUD[ , -c(34:842)])
colnames(SSTUD) <- c('Longitude', 'Latitude', paste0('PC', 1:33))

## Cluster
set.seed(662321)
SVDclustres <- kmeans(SSTUD[, -c(1, 2)], 2, nstart = 20, iter.max = 15)
SVDclustout <- data.frame(LongLat, Cluster = factor(SVDclustres$cluster))
colnames(SVDclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

```{r loadothers, include=FALSE}
if (file.exists('SSTdimreduce.RData')){
    load('SSTdimreduce.RData')
} else {
    kpca2 <- kpca(MonthlySST, kernel = "vanilladot", kpar = list())
    kpcarand <- kpca(rand, kernel = "vanilladot", kpar = list())
    lleres <- lle(MonthlySST, m = 2, k = 16)
    LE <- embed(MonthlySST, .method = 'LaplacianEigenmaps')
    save(kpca2, kpcarand, lleres, LE, file = 'SSTdimreduce.RData')
}
```

\ \ \ \ \ \ \ \ 
First, the SST data set is converted into principal components (PCs) using SVD. The number of important components is assessed by looking at the number of components with singular values above those that are generated from a randomized data set. Singular values for each component for the true and random data set can be seen in Figure \ref{fig:eigenfig}. This suggests using 33 PCs which accounts for a total variance of `r round(SVDvaracc[33], 2)`%with the first component accounting for `r round(SVDvaracc[1], 2)`%. Figure \ref{fig:SSTPC1s} plots the weights associated with the first PC on the latitude and longitude values. 

\ \ \ \ \ \ \ \ \ 
Of the indices for determining the number of clusters from the suggested 33 PCs from the PCA, eight suggest two clusters, seven suggest three clusters, and seven suggest 20 clusters. Inspection of the clusters when plotted with latitude and longitude show a close similarity between having 2 and 3 clusters. The two cluster solution can be seen in Figure \ref{fig:SSTClusters} The addition of another cluster just adds another area around the central cluster in the two cluster solution. Using 20 clusters seems a little excessive in this case and lead to some very small areas that belonged to one cluster but were not near the main cluster.


```{r SSTKPCA, include = FALSE}
## Something happened here.... no longer getting the same clusters even with seed...

#kpca2 <- kpca(MonthlySST, kernel = "vanilladot", kpar = list()) 
SSTkpcavar <- cumsum(kpca2@eig[1:32]^2)/sum(kpca2@eig^2)*100
#kpcarand <- kpca(rand, kernel = "vanilladot", kpar = list())
kpcaeig <- data.frame(Component = 1:162, Real = kpca2@eig, Random = kpcarand@eig[1:162])
colnames(kpcaeig) <- c('Component', 'Real Data Trace', 'Random Data Trace')
kpcaeig<- melt(kpcaeig, id.vars = 'Component', 
                    variable.name = 'Data', value.name = 'EigenVal')


KPCApcs <- kpca2@pcv[ , 1:6]
KPCApcs <- data.frame(LongLat, KPCApcs)
colnames(KPCApcs) <- c('Longitude', 'Latitude', paste0('PC', 1:6))

set.seed(662321)
## Suggests 5
KPCAclustres <- kmeans(KPCApcs[, -c(1,2)], 5, nstart = 20, iter.max = 15) 
KPCAclustout <- data.frame(LongLat, Cluster = factor(KPCAclustres$cluster))
colnames(KPCAclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

\ \ \ \ \ \ \ \ 
A similar procedure was followed for the KPCA except multiple kernels were also assessed. Of these, the linear kernel function seems to perform the most efficiently. It takes fewer PCs and less tuning to account for a similar amount of variance as the other kernels do. Similarly, when comparing to random data, 32 components are suggested this time (Figure \ref{fig:eigenfig}). However, if we use the SVD as a base line, it is possible that only 6 components are needed because they account for `r round(SSTkpcavar[6], 2)`% of the variance. Figure \ref{fig:SSTPC1s} presents a plot of the first PC for the KPCA. 

\ \ \ \ \ \ \ \ 
Using these six components for clustering, seven indices suggest five as the best number of clusters, four suggest two clusters, and 3 suggest 6 or 20 clusters. So, five clusters seems to have the most support. The cluster structure here seems to break up the northern and eastern most sections of the pacific more with a bit of an alternating pattern, leaving a single cluster for the southwest area. The clustering can be seen in Figure \ref{fig:SSTClusters}.

```{r eigenfigs, echo=FALSE, results='asis', fig.cap = '\\label{fig:eigenfig} Plot of singular values and Eigen values for real and randomized data set for PCA and KPCA respectively. The PCs of the start performing worse than random after component 33 for the PCA and 32 for KPCA.', fig.align = 'center', out.width='90%'}
## Suggests 33 PCs (97% Var)
SSTsingvals <- melt(SSTsingvals, id.vars = 'Component', 
                    variable.name = 'Data', value.name = 'SingVal')

singvalplot <- ggplot(SSTsingvals, aes(x = Component, y = SingVal, color = Data)) + 
               geom_line() + form + xlab('Principal Component') + ylab('Singular Value') + labs(fill = 'Data Set') + 
               theme(legend.position = c(.8, .8)) + ggtitle('PCA Singular Values')
    

    
## If legend needs resizing
# theme(legend.position = c(.8, .8), legend.key.size = unit(1, 'cm'),
#                                          legend.key.height = unit(1, 'cm'), 
#                                          legend.key.width = unit(1, 'cm'), 
#                                          legend.title = element_text(size=14), 
#                                          legend.text = element_text(size=12)) 

## Suggests 33 PCs (97% Var)
eigenplot <- ggplot(kpcaeig, aes(x = Component, y = EigenVal, color = Data)) + 
             geom_line() + form + xlab('Principal Component') + ylab('Eigen Value') + labs(fill = 'Data Set') + 
             theme(legend.position = c(.8, .8)) + ggtitle('KPCA Eigen Values')

ggarrange(singvalplot, eigenplot, ncol = 1, nrow = 2)
```

```{r SSTPC1s, echo=FALSE, results='asis', fig.cap = '\\label{fig:SSTPC1s} Plot of PC1 from PCA and KPCA of SST data.', fig.align = 'center'}
## Plot PC 1
SSTSVDPC1 <- ggplot(SSTUD, aes(x = Longitude, y = Latitude, fill = PC1)) + 
             coord_fixed(ratio = 1) + geom_raster(alpha = 1) + form + 
             scale_fill_gradientn(na.value = "white", colours = c("yellow", "orange", "green", "blue")) +
             ggtitle('PCA PC1')

## Plot PC 1
SSTkpcaPC1 <- ggplot(KPCApcs, aes(x = Longitude, y = Latitude, fill = PC1)) + 
              coord_fixed(ratio = 1) + geom_raster(alpha = 1) + form + 
              scale_fill_gradientn(na.value = "white", colours = c("yellow", "orange", "green", "blue")) +
              ggtitle('KPCA PC1')

ggarrange(SSTSVDPC1, SSTkpcaPC1, ncol = 1, nrow = 2)
```

```{r SSTLLE, include=FALSE}
# lleres <- lle(MonthlySST, m = 2, k = 16)
# LE <- embed(MonthlySST, .method = 'LaplacianEigenmaps')

Ymat <- data.frame(lleres$Y)
LEout <- data.frame(Obs = 1:2512, LE@data@data)

set.seed(662321)
LLEclustres <- kmeans(Ymat, 10, nstart = 20, iter.max = 15)
LLEclustout <- data.frame(LongLat, Cluster = factor(LLEclustres$cluster))
colnames(LLEclustout) <- c('Longitude', 'Latitude', 'Cluster')

set.seed(662321)
LEclustres <- kmeans(LEout[, -1], 3, nstart = 20, iter.max = 15) ## suggests 2 or 3 clusters
LEclustout <- factor(LEclustres$cluster)
LEclustout <- data.frame(LongLat, Cluster = factor(LEclustres$cluster))
colnames(LEclustout) <- c('Longitude', 'Latitude', 'Cluster')
```

\ \ \ \ \ \ \ \ 
Dimensional reduction was also attempted using LLE and Laplacian Eigenmaps. For LLE, the optimum number of neighbors found for two intrinsic dimensions is 16. The proposed number of clusters by the indices is 10. This was suggest by six indices, followed closely by three clusters with five indices and two clusters with four indices. This is by far the most chaotic clustering seen in Figure \ref{fig:SSTClusters}. Many of the clusters are surround by other clusters.

\ \ \ \ \ \ \ \ 
The Laplacian Eigenmap also reduced dimensionality down to two. Clustering indices suggest that the optimal number of clusters is three with six indices. Two and four clusters could also be considered with 4 indices proposing each of those. Using three clusters, the Pacific is split into a single cluster to the west and two clusters in the east being split almost at the equator (Figure \ref{fig:SSTClusters}). 

```{r clusterplot1, echo = FALSE, results='asis', fig.cap = '\\label{fig:SSTClusters} Best clustering solutions for each of the dimension reduction methods for SST data.', fig.align = 'center', fig.height=9}
## SVD plot
SSTSVDClust <- ggplot(SVDclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
               coord_fixed(ratio = 1) + geom_raster(alpha = 1) + form + ggtitle('Clustering from PCA') +
                theme(legend.key.size = unit(.5, 'cm'),
                     legend.key.height = unit(.5, 'cm'),
                     legend.key.width = unit(.5, 'cm'),
                     legend.title = element_text(size=10),
                     legend.text = element_text(size=8)) 

## KPCA plot
SSTKPCAClust <- ggplot(KPCAclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) +
                coord_fixed(ratio = 1) + geom_raster(alpha = 1) + form + ggtitle('Clustering from KPCA') +
                theme(legend.key.size = unit(.5, 'cm'),
                     legend.key.height = unit(.5, 'cm'),
                     legend.key.width = unit(.5, 'cm'),
                     legend.title = element_text(size=10),
                     legend.text = element_text(size=8)) 

## LLE plot
SSTLLEClust <- ggplot(LLEclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
               coord_fixed(ratio = 1) + geom_raster(alpha = 1) + form + ggtitle('Clustering from LLE') +
               theme(legend.key.size = unit(.5, 'cm'),
                     legend.key.height = unit(.5, 'cm'),
                     legend.key.width = unit(.5, 'cm'),
                     legend.title = element_text(size=10),
                     legend.text = element_text(size=8)) + guides(fill=guide_legend(ncol=2))

## LE plot
SSTLEClust <- ggplot(LEclustout, aes(x = Longitude, y = Latitude,  fill = Cluster)) + 
              coord_fixed(ratio = 1) + geom_raster(alpha = 1) + form + ggtitle('Clustering from LE') +
              theme(legend.key.size = unit(.5, 'cm'),
                     legend.key.height = unit(.5, 'cm'),
                     legend.key.width = unit(.5, 'cm'),
                     legend.title = element_text(size=10),
                     legend.text = element_text(size=8)) 

ggarrange(SSTSVDClust, SSTKPCAClust, SSTLLEClust, SSTLEClust, ncol = 1, nrow = 4)
```

```{r clustbypc, echo = FALSE, results='asis', fig.cap = '\\label{fig:SSTClustbypc} Location clusters plotted by the first two PCs or manifolds of each dimension reduction techinque.', fig.align = 'center'}
SSTSVDPCplot <- ggplot(data = data.frame(SSTUD, SVDclustout), aes(x = PC1, y = PC2, color = Cluster)) + 
                geom_point(size = .3) + form + xlab('Principal Component 1') + 
                ylab('Principal Component 2') + ggtitle('PCA Clusters') +
                theme(legend.key.size = unit(.25, 'cm'),
                     legend.key.height = unit(.25, 'cm'),
                     legend.key.width = unit(.25, 'cm'),
                     legend.title = element_text(size=8),
                     legend.text = element_text(size=6)) 

SSTKPCAPCplot <- ggplot(data = data.frame(KPCApcs, KPCAclustout), 
                        aes(x = PC1, y = PC2, color = Cluster)) + 
                 geom_point(size = .3) + form + xlab('Principal Component 1') + 
                 ylab('Principal Component 2') + ggtitle('KPCA Clusters') +
                 theme(legend.key.size = unit(.25, 'cm'),
                     legend.key.height = unit(.25, 'cm'),
                     legend.key.width = unit(.25, 'cm'),
                     legend.title = element_text(size=8),
                     legend.text = element_text(size=6)) 

SSTLLEdimplot <- ggplot(data = data.frame(Ymat, LLEclustout), aes(x = Ymat[, 1], y = Ymat[, 2], color = Cluster)) +
                 geom_point(size = .3) + form + xlab('Dimension 1') + 
                 ylab('Dimension 2') + ggtitle('LLE Clusters') +
                 theme(legend.key.size = unit(.25, 'cm'),
                     legend.key.height = unit(.25, 'cm'),
                     legend.key.width = unit(.25, 'cm'),
                     legend.title = element_text(size=8),
                     legend.text = element_text(size=6)) + guides(fill=guide_legend(ncol=2))

SSTLEmanifolds <- ggplot(data = data.frame(LEout, LEclustout), aes(x = LEout[, 1], y = LEout[, 2], color = Cluster)) +
                  geom_point(size = .3) + form + xlab('Manifold 1') + 
                  ylab('Manifold 2') + ggtitle('LE Clusters') +
                  theme(legend.key.size = unit(.25, 'cm'),
                     legend.key.height = unit(.25, 'cm'),
                     legend.key.width = unit(.25, 'cm'),
                     legend.title = element_text(size=8),
                     legend.text = element_text(size=6)) 

ggarrange(SSTSVDPCplot, SSTKPCAPCplot, SSTLLEdimplot, SSTLEmanifolds, ncol = 2, nrow = 2)
```

### Precipitation Results